{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AN√ÅLISE AVAN√áADA DE ABSENTISMO\n",
    "\n",
    "**Framework Anal√≠tico Completo**: Da Descri√ß√£o √† Prescri√ß√£o\n",
    "\n",
    "---\n",
    "\n",
    "## Estrutura da An√°lise\n",
    "\n",
    "1. **Prepara√ß√£o e Limpeza de Dados**\n",
    "2. **Descri√ß√£o Fundamental dos Dados**\n",
    "3. **Conceito de Spells (Epis√≥dios de Aus√™ncia)**\n",
    "4. **M√©tricas Core (KPIs Essenciais)**\n",
    "5. **Bradford Factor Analysis**\n",
    "6. **Dete√ß√£o de Padr√µes Suspeitos**\n",
    "7. **An√°lise de Cohorts (por Data de Ingresso)**\n",
    "8. **Clustering de Perfis de Absentismo**\n",
    "9. **Network Analysis (Coincid√™ncias)**\n",
    "10. **Event Detection & Anomaly Detection**\n",
    "11. **Visualiza√ß√µes Avan√ßadas**\n",
    "12. **S√≠ntese Executiva e A√ß√µes Recomendadas**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. PREPARA√á√ÉO E LIMPEZA DE DADOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configura√ß√µes de visualiza√ß√£o\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "print('‚úì Bibliotecas carregadas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 Carregar dados\n",
    "print('Carregando dados...')\n",
    "df_raw = pd.read_csv('combined_data.csv')\n",
    "df_raw['Data'] = pd.to_datetime(df_raw['Data'])\n",
    "\n",
    "print(f'   Registos totais: {len(df_raw):,}')\n",
    "print(f'   Colaboradores √∫nicos: {df_raw[\"login_colaborador\"].nunique():,}')\n",
    "print(f'   Per√≠odo: {df_raw[\"Data\"].min().date()} at√© {df_raw[\"Data\"].max().date()}')\n",
    "\n",
    "# Carregar NOVOS c√≥digos (V2)\n",
    "print('\\nCarregando nova classifica√ß√£o (C√≥digos_V2)...')\n",
    "df_codigos = pd.read_excel('c√≥digos_V2.xlsx')\n",
    "print(f'   C√≥digos carregados: {len(df_codigos)}')\n",
    "print(f'\\nNivel 1 categories: {df_codigos[\"Nivel 1\"].nunique()}')\n",
    "print(df_codigos['Nivel 1'].value_counts())\n",
    "print(f'\\nNivel 2 categories: {df_codigos[\"Nivel 2\"].nunique()}')\n",
    "print(df_codigos['Nivel 2'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2 Merge com novos c√≥digos\n",
    "print('Aplicando nova classifica√ß√£o aos dados...')\n",
    "\n",
    "# Merge\n",
    "df_raw = df_raw.merge(\n",
    "    df_codigos,\n",
    "    left_on='segmento_processado_codigo',\n",
    "    right_on='Codigo Segmento',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Verificar c√≥digos n√£o mapeados\n",
    "codigos_sem_match = df_raw[df_raw['Nivel 1'].isna()]['segmento_processado_codigo'].unique()\n",
    "if len(codigos_sem_match) > 0:\n",
    "    print(f'\\n‚ö†Ô∏è  ATEN√á√ÉO: {len(codigos_sem_match)} c√≥digos sem correspond√™ncia:')\n",
    "    print(codigos_sem_match)\n",
    "    print(f'   Registos afetados: {df_raw[\"Nivel 1\"].isna().sum():,}')\n",
    "else:\n",
    "    print('\\n‚úì Todos os c√≥digos mapeados com sucesso')\n",
    "\n",
    "print(f'\\nTotal de registos: {len(df_raw):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Identificar e Remover Incompatibilidades\n",
    "\n",
    "**Particularidade dos dados**: Num mesmo dia, um colaborador pode ter **m√∫ltiplos registos**.\n",
    "\n",
    "**Regras de Compatibilidade** (Nivel 2):\n",
    "- ‚úÖ **Atraso** + Presen√ßa\n",
    "- ‚úÖ **Atraso** + Exame Escolar  \n",
    "- ‚úÖ **Presen√ßa** + Exame Escolar\n",
    "- ‚ùå **Tudo o resto √© INCOMPAT√çVEL**\n",
    "\n",
    "Vamos criar uma matriz de compatibilidade e identificar dias problem√°ticos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3.1 Criar matriz de compatibilidade (Nivel 2)\n",
    "print('Criando matriz de compatibilidade (Nivel 2)...')\n",
    "\n",
    "# Lista de categorias Nivel 2\n",
    "lista_nivel2 = sorted(df_codigos['Nivel 2'].unique())\n",
    "print(f'\\nCategorias Nivel 2: {len(lista_nivel2)}')\n",
    "for cat in lista_nivel2:\n",
    "    print(f'  - {cat}')\n",
    "\n",
    "# Criar matriz (default: INCOMPAT√çVEL)\n",
    "matriz_compat = pd.DataFrame('Incompat√≠vel', index=lista_nivel2, columns=lista_nivel2)\n",
    "\n",
    "# REGRA 1: Categoria consigo mesma = COMPAT√çVEL\n",
    "for cat in lista_nivel2:\n",
    "    matriz_compat.loc[cat, cat] = 'Compat√≠vel'\n",
    "\n",
    "# REGRA 2: Apenas 3 pares compat√≠veis\n",
    "pares_compativeis = [\n",
    "    ('Atraso', 'Presen√ßa'),\n",
    "    ('Atraso', 'Exame Escolar'),\n",
    "    ('Presen√ßa', 'Exame Escolar')\n",
    "]\n",
    "\n",
    "for cat1, cat2 in pares_compativeis:\n",
    "    if cat1 in matriz_compat.index and cat2 in matriz_compat.columns:\n",
    "        matriz_compat.loc[cat1, cat2] = 'Compat√≠vel'\n",
    "        matriz_compat.loc[cat2, cat1] = 'Compat√≠vel'  # Sim√©trica\n",
    "\n",
    "# Exportar matriz para valida√ß√£o\n",
    "with pd.ExcelWriter('matriz_compatibilidade_nivel2_v2.xlsx') as writer:\n",
    "    matriz_compat.to_excel(writer, sheet_name='Matriz')\n",
    "    \n",
    "print('\\n‚úì Matriz de compatibilidade criada e exportada')\n",
    "print(f'\\nPares COMPAT√çVEIS (al√©m da diagonal):' )\n",
    "for cat1, cat2 in pares_compativeis:\n",
    "    print(f'  ‚úì {cat1} + {cat2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3.2 Identificar dias com m√∫ltiplos registos\n",
    "print('Identificando dias com m√∫ltiplos registos...')\n",
    "\n",
    "registros_por_dia = df_raw.groupby(['login_colaborador', 'Data']).size()\n",
    "dias_duplicados = registros_por_dia[registros_por_dia > 1]\n",
    "\n",
    "print(f'   Dias com m√∫ltiplos registos: {len(dias_duplicados):,}')\n",
    "print(f'   ({len(dias_duplicados) / len(df_raw.groupby([\"login_colaborador\", \"Data\"])) * 100:.2f}% do total de dias-colaborador)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3.3 Testar incompatibilidades (VERS√ÉO OTIMIZADA)\n",
    "print('Testando incompatibilidades nos dias duplicados...')\n",
    "\n",
    "# PR√â-FILTRAR apenas dias duplicados\n",
    "dias_dup_list = dias_duplicados.index.tolist()\n",
    "df_dup = df_raw.set_index(['login_colaborador', 'Data']).loc[dias_dup_list].reset_index()\n",
    "\n",
    "print(f'   Registos a testar: {len(df_dup):,}')\n",
    "\n",
    "# PR√â-AGRUPAR dados (UMA vez!)\n",
    "df_dup_grouped = df_dup.groupby(['login_colaborador', 'Data']).apply(\n",
    "    lambda g: pd.Series({\n",
    "        'categorias_nivel2': list(g['Nivel 2'].dropna().unique()),\n",
    "        'codigos': list(g['segmento_processado_codigo'].unique()),\n",
    "        'nome': g['nome_colaborador'].iloc[0]\n",
    "    })\n",
    ").reset_index()\n",
    "\n",
    "print(f'   Dias √∫nicos a testar: {len(df_dup_grouped):,}')\n",
    "\n",
    "# Identificar incompatibilidades\n",
    "print('\\nTestando pares de categorias...')\n",
    "dias_incompativeis = []\n",
    "\n",
    "for idx, row in df_dup_grouped.iterrows():\n",
    "    if idx % 5000 == 0 and idx > 0:\n",
    "        print(f'   Processados {idx:,}/{len(df_dup_grouped):,} dias...')\n",
    "    \n",
    "    login = row['login_colaborador']\n",
    "    data = row['Data']\n",
    "    categorias = row['categorias_nivel2']\n",
    "    codigos = row['codigos']\n",
    "    nome = row['nome']\n",
    "    \n",
    "    # Testar todos os pares\n",
    "    incompativel_encontrado = False\n",
    "    pares_incompativeis = []\n",
    "    \n",
    "    for i, cat1 in enumerate(categorias):\n",
    "        for cat2 in categorias[i+1:]:\n",
    "            if cat1 in matriz_compat.index and cat2 in matriz_compat.columns:\n",
    "                if matriz_compat.loc[cat1, cat2] == 'Incompat√≠vel':\n",
    "                    incompativel_encontrado = True\n",
    "                    pares_incompativeis.append(f'{cat1} + {cat2}')\n",
    "    \n",
    "    if incompativel_encontrado:\n",
    "        dias_incompativeis.append({\n",
    "            'login_colaborador': login,\n",
    "            'Data': data,\n",
    "            'nome_colaborador': nome,\n",
    "            'categorias': ', '.join(categorias),\n",
    "            'codigos': ', '.join(codigos),\n",
    "            'pares_incompativeis': ' | '.join(pares_incompativeis)\n",
    "        })\n",
    "\n",
    "df_incompativeis = pd.DataFrame(dias_incompativeis)\n",
    "\n",
    "print(f'\\n‚úì Teste conclu√≠do')\n",
    "print(f'\\nüî¥ INCOMPATIBILIDADES ENCONTRADAS: {len(df_incompativeis)}')\n",
    "\n",
    "if len(df_incompativeis) > 0:\n",
    "    print(f'\\nDistribui√ß√£o por par incompat√≠vel:')\n",
    "    for par in df_incompativeis['pares_incompativeis'].str.split(' | ').explode().value_counts().head(10).items():\n",
    "        print(f'   {par[0]}: {par[1]} casos')\n",
    "    \n",
    "    # Exportar para Excel\n",
    "    df_incompativeis.to_excel('incompatibilidades_encontradas_v2.xlsx', index=False)\n",
    "    print('\\n‚úì Detalhes exportados para: incompatibilidades_encontradas_v2.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3.4 Remover dias incompat√≠veis (VERS√ÉO VETORIZADA)\n",
    "print('Removendo dias incompat√≠veis...')\n",
    "\n",
    "n_antes = len(df_raw)\n",
    "\n",
    "if len(df_incompativeis) > 0:\n",
    "    # Criar DataFrame dos dias a remover\n",
    "    dias_remover = df_incompativeis[['login_colaborador', 'Data']].copy()\n",
    "    \n",
    "    # Merge com indicador\n",
    "    df_temp = df_raw.merge(dias_remover, on=['login_colaborador', 'Data'], how='left', indicator=True)\n",
    "    \n",
    "    # Manter apenas linhas N√ÉO marcadas\n",
    "    df_limpo = df_temp[df_temp['_merge'] == 'left_only'].drop('_merge', axis=1)\n",
    "else:\n",
    "    df_limpo = df_raw.copy()\n",
    "    print('   Nenhuma incompatibilidade encontrada')\n",
    "\n",
    "n_depois = len(df_limpo)\n",
    "print(f'\\n‚úì Registos removidos: {n_antes - n_depois:,}')\n",
    "print(f'   ({(n_antes - n_depois) / n_antes * 100:.3f}% do total)')\n",
    "print(f'\\nDataset limpo: {n_depois:,} registos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# APLICAR REGRAS DE HIERARQUIA - VERS√ÉO R√ÅPIDA\n",
    "\n",
    "print('=== APLICANDO REGRAS DE HIERARQUIA ===\\n')\n",
    "\n",
    "# ============================================================================\n",
    "# PR√â-AGRUPAR para identificar combina√ß√µes (R√ÅPIDO)\n",
    "# ============================================================================\n",
    "print('Identificando combina√ß√µes...')\n",
    "\n",
    "dias_mult = df_limpo.groupby(['login_colaborador', 'Data']).size()\n",
    "dias_mult = dias_mult[dias_mult > 1].reset_index()\n",
    "dias_mult.columns = ['login_colaborador', 'Data', 'num_registos']\n",
    "\n",
    "# PR√â-FILTRAR s√≥ dias m√∫ltiplos\n",
    "dias_mult_list = list(zip(dias_mult['login_colaborador'], dias_mult['Data']))\n",
    "df_mult = df_limpo[df_limpo.set_index(['login_colaborador', 'Data']).index.isin(dias_mult_list)].copy()\n",
    "\n",
    "# PR√â-AGRUPAR (uma vez s√≥!)\n",
    "df_comb = df_mult.groupby(['login_colaborador', 'Data']).agg({\n",
    "    'Nivel 1': lambda x: ' + '.join(sorted(x.dropna().unique())),\n",
    "    'Nivel 2': lambda x: ' + '.join(sorted(x.dropna().unique()))\n",
    "}).reset_index()\n",
    "\n",
    "df_comb.columns = ['login_colaborador', 'Data', 'combinacao_nivel1', 'combinacao_nivel2']\n",
    "\n",
    "print(f'   ‚úì Combina√ß√µes: {len(df_comb):,}')\n",
    "\n",
    "# ============================================================================\n",
    "# ELIMINAR CASOS\n",
    "# ============================================================================\n",
    "print('\\n1. Eliminando casos raros...')\n",
    "\n",
    "# Nivel 1\n",
    "eliminar_n1 = df_comb[df_comb['combinacao_nivel1'].isin([\n",
    "    'Falta Injustificada + Trabalho Pago',\n",
    "    'Falta Justificada + Trabalho Pago',\n",
    "    'Atraso + Falta Justificada'\n",
    "])]\n",
    "\n",
    "# Nivel 2\n",
    "eliminar_n2 = df_comb[df_comb['combinacao_nivel2'].str.contains(\n",
    "    'Feriado \\+ Presen√ßa|F√©rias \\+ Presen√ßa|Folga \\+ Presen√ßa|Falta Injustificada \\+ Presen√ßa|Assist√™ncia Familiar \\+ Presen√ßa|Atraso \\+ Aus√™ncia M√©dica|Aus√™ncia Justificada \\+ Presen√ßa',\n",
    "    na=False, regex=True\n",
    ")]\n",
    "\n",
    "eliminar_total = pd.concat([eliminar_n1, eliminar_n2]).drop_duplicates()\n",
    "\n",
    "print(f'   Casos a eliminar: {len(eliminar_total)}')\n",
    "\n",
    "# Eliminar (criar lista de √≠ndices)\n",
    "idx_eliminar = df_limpo.set_index(['login_colaborador', 'Data']).index.isin(\n",
    "    list(zip(eliminar_total['login_colaborador'], eliminar_total['Data']))\n",
    ")\n",
    "df_limpo = df_limpo[~idx_eliminar]\n",
    "\n",
    "print(f'   ‚úì Eliminados')\n",
    "\n",
    "# ============================================================================\n",
    "# SEPARAR ATRASOS\n",
    "# ============================================================================\n",
    "print('\\n2. Separando atrasos...')\n",
    "\n",
    "dias_atraso = df_comb[df_comb['combinacao_nivel1'] == 'Atraso + Trabalho Pago']\n",
    "\n",
    "print(f'   Dias Atraso + Trabalho Pago: {len(dias_atraso):,}')\n",
    "\n",
    "# Criar df_atrasos (copiar linhas onde Nivel 1 = Atraso)\n",
    "idx_atrasos = df_limpo.set_index(['login_colaborador', 'Data']).index.isin(\n",
    "    list(zip(dias_atraso['login_colaborador'], dias_atraso['Data']))\n",
    ") & (df_limpo['Nivel 1'] == 'Atraso')\n",
    "\n",
    "df_atrasos = df_limpo[idx_atrasos].copy()\n",
    "\n",
    "print(f'   ‚úì df_atrasos: {len(df_atrasos):,} registos')\n",
    "\n",
    "# Remover atrasos do principal\n",
    "df_limpo = df_limpo[~idx_atrasos]\n",
    "\n",
    "print(f'   ‚úì Removidos do principal')\n",
    "\n",
    "# ============================================================================\n",
    "# AGREGAR\n",
    "# ============================================================================\n",
    "print('\\n3. Agregando...')\n",
    "\n",
    "agg_rules = {\n",
    "    'nome_colaborador': 'first',\n",
    "    'categoria_profissional': 'first',\n",
    "    'segmento_processado_codigo': lambda x: ', '.join(sorted(x.unique())),\n",
    "    'Nivel 1': lambda x: ', '.join(sorted(x.dropna().unique())),\n",
    "    'Nivel 2': lambda x: ', '.join(sorted(x.dropna().unique())),\n",
    "}\n",
    "\n",
    "if 'operacao' in df_limpo.columns:\n",
    "    agg_rules['operacao'] = 'first'\n",
    "if 'Activo?' in df_limpo.columns:\n",
    "    agg_rules['Activo?'] = 'first'\n",
    "if 'DtActivacao' in df_limpo.columns:\n",
    "    agg_rules['DtActivacao'] = 'first'\n",
    "if 'DtDesactivacao' in df_limpo.columns:\n",
    "    agg_rules['DtDesactivacao'] = 'first'\n",
    "\n",
    "df = df_limpo.groupby(['login_colaborador', 'Data']).agg(agg_rules).reset_index()\n",
    "print(f'   ‚úì df: {len(df):,}')\n",
    "\n",
    "if len(df_atrasos) > 0:\n",
    "    df_atrasos = df_atrasos.groupby(['login_colaborador', 'Data']).agg(agg_rules).reset_index()\n",
    "    print(f'   ‚úì df_atrasos: {len(df_atrasos):,}')\n",
    "\n",
    "print('\\n' + '='*70)\n",
    "print(f'CONCLU√çDO - df: {len(df):,} | df_atrasos: {len(df_atrasos):,}')\n",
    "print('='*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. DESCRI√á√ÉO FUNDAMENTAL DOS DADOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 Estat√≠sticas descritivas\n",
    "print('=== DESCRI√á√ÉO DO DATASET ===')\n",
    "print(f'\\nüìä DIMENS√ÉO')\n",
    "print(f'   Total de registos (dias-colaborador): {len(df):,}')\n",
    "print(f'   Colaboradores √∫nicos: {df[\"login_colaborador\"].nunique():,}')\n",
    "print(f'   Per√≠odo: {df[\"Data\"].min().date()} at√© {df[\"Data\"].max().date()}')\n",
    "print(f'   Dias calend√°rio: {(df[\"Data\"].max() - df[\"Data\"].min()).days + 1}')\n",
    "\n",
    "print(f'\\nüìã DISTRIBUI√á√ÉO POR NIVEL 1')\n",
    "dist_nivel1 = df['Nivel 1'].value_counts()\n",
    "for cat, count in dist_nivel1.items():\n",
    "    pct = count / len(df) * 100\n",
    "    print(f'   {cat:25s}: {count:7,} ({pct:5.2f}%)')\n",
    "\n",
    "print(f'\\nüìã DISTRIBUI√á√ÉO POR NIVEL 2')\n",
    "dist_nivel2 = df['Nivel 2'].value_counts()\n",
    "for cat, count in dist_nivel2.head(15).items():\n",
    "    pct = count / len(df) * 100\n",
    "    print(f'   {cat:30s}: {count:7,} ({pct:5.2f}%)')\n",
    "\n",
    "if 'operacao' in df.columns:\n",
    "    print(f'\\nüè¢ OPERA√á√ïES')\n",
    "    print(f'   Opera√ß√µes √∫nicas: {df[\"operacao\"].nunique()}')\n",
    "    print(f'   Top 10 opera√ß√µes:')\n",
    "    for op, count in df['operacao'].value_counts().head(10).items():\n",
    "        pct = count / len(df) * 100\n",
    "        print(f'      {op:40s}: {count:6,} ({pct:5.2f}%)')\n",
    "\n",
    "print(f'\\nüë• CATEGORIAS PROFISSIONAIS')\n",
    "print(f'   Categorias √∫nicas: {df[\"categoria_profissional\"].nunique()}')\n",
    "for cat, count in df['categoria_profissional'].value_counts().head(10).items():\n",
    "    pct = count / len(df) * 100\n",
    "    print(f'   {cat:30s}: {count:6,} ({pct:5.2f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 Visualiza√ß√£o: Distribui√ß√£o temporal\n",
    "print('Criando visualiza√ß√£o de distribui√ß√£o temporal...')\n",
    "\n",
    "# Agrupar por m√™s\n",
    "df['Ano_Mes'] = df['Data'].dt.to_period('M').astype(str)\n",
    "df_mensal = df.groupby('Ano_Mes').size().reset_index(name='Registos')\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=df_mensal['Ano_Mes'],\n",
    "    y=df_mensal['Registos'],\n",
    "    mode='lines+markers',\n",
    "    name='Registos',\n",
    "    line=dict(color='#1f77b4', width=2),\n",
    "    marker=dict(size=6)\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Evolu√ß√£o Temporal dos Registos',\n",
    "    xaxis_title='M√™s',\n",
    "    yaxis_title='N√∫mero de Registos',\n",
    "    height=400,\n",
    "    hovermode='x unified'\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "print('‚úì Visualiza√ß√£o criada')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. CONCEITO DE SPELLS (EPIS√ìDIOS DE AUS√äNCIA)\n",
    "\n",
    "**Spell** = Epis√≥dio cont√≠nuo de aus√™ncia (um ou mais dias consecutivos).\n",
    "\n",
    "**Import√¢ncia**:\n",
    "- 1 spell de 5 dias vs 5 spells de 1 dia = **impacto operacional MUITO diferente**\n",
    "- Permite calcular m√©tricas sofisticadas:\n",
    "  - **Frequency Rate**: Taxa de spells por colaborador\n",
    "  - **Mean Spell Duration**: Dura√ß√£o m√©dia de cada epis√≥dio\n",
    "  - **Short-term spells**: ‚â§ 3 dias (poss√≠vel \"abuso\")\n",
    "  - **Long-term spells**: > 14 dias (doen√ßa grave, tratamento m√©dico)\n",
    "\n",
    "**Algoritmo**:\n",
    "1. Ordenar registos por colaborador e data\n",
    "2. Identificar quebras de continuidade (gap > 1 dia)\n",
    "3. Agrupar dias consecutivos no mesmo spell\n",
    "4. Calcular dura√ß√£o, tipo predominante, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Criar spells de AUS√äNCIA (excluir Presen√ßas e Atrasos)\n",
    "print('Criando spells de aus√™ncia...')\n",
    "\n",
    "# Filtrar apenas AUS√äNCIAS (excluir Trabalho Pago, Atraso)\n",
    "df_ausencias = df[df['Nivel 1'].isin(['Falta Justificada', 'Falta Injustificada', 'Aus√™ncia'])].copy()\n",
    "\n",
    "print(f'   Registos de aus√™ncia: {len(df_ausencias):,}')\n",
    "print(f'   Colaboradores com aus√™ncias: {df_ausencias[\"login_colaborador\"].nunique():,}')\n",
    "\n",
    "# Ordenar por colaborador e data\n",
    "df_ausencias = df_ausencias.sort_values(['login_colaborador', 'Data']).reset_index(drop=True)\n",
    "\n",
    "# Calcular diferen√ßa de dias entre registos consecutivos\n",
    "df_ausencias['dias_desde_anterior'] = df_ausencias.groupby('login_colaborador')['Data'].diff().dt.days\n",
    "\n",
    "# Novo spell quando:\n",
    "# 1. Primeiro registo do colaborador (dias_desde_anterior = NaN)\n",
    "# 2. Gap > 1 dia (n√£o consecutivo)\n",
    "df_ausencias['novo_spell'] = (\n",
    "    (df_ausencias['dias_desde_anterior'].isna()) |  # Primeiro registo\n",
    "    (df_ausencias['dias_desde_anterior'] > 1)        # Gap de dias\n",
    ")\n",
    "\n",
    "# Atribuir ID √∫nico a cada spell\n",
    "df_ausencias['spell_id'] = df_ausencias['novo_spell'].cumsum()\n",
    "\n",
    "print(f'\\n‚úì Spells identificados: {df_ausencias[\"spell_id\"].nunique():,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 Agregar informa√ß√£o por spell\n",
    "print('Agregando informa√ß√£o por spell...')\n",
    "\n",
    "df_spells = df_ausencias.groupby('spell_id').agg({\n",
    "    'login_colaborador': 'first',\n",
    "    'nome_colaborador': 'first',\n",
    "    'categoria_profissional': 'first',\n",
    "    'Data': ['min', 'max', 'count'],  # In√≠cio, fim, dura√ß√£o\n",
    "    'Nivel 1': lambda x: x.mode()[0] if len(x.mode()) > 0 else x.iloc[0],  # Tipo predominante\n",
    "    'Nivel 2': lambda x: x.mode()[0] if len(x.mode()) > 0 else x.iloc[0],\n",
    "}).reset_index()\n",
    "\n",
    "# Renomear colunas\n",
    "df_spells.columns = [\n",
    "    'spell_id', 'login_colaborador', 'nome_colaborador', 'categoria_profissional',\n",
    "    'data_inicio', 'data_fim', 'duracao_dias',\n",
    "    'nivel1_predominante', 'nivel2_predominante'\n",
    "]\n",
    "\n",
    "# Adicionar opera√ß√£o se existir\n",
    "if 'operacao' in df_ausencias.columns:\n",
    "    df_spells = df_spells.merge(\n",
    "        df_ausencias.groupby('spell_id')['operacao'].first().reset_index(),\n",
    "        on='spell_id'\n",
    "    )\n",
    "\n",
    "# Adicionar features\n",
    "df_spells['dia_semana_inicio'] = df_spells['data_inicio'].dt.day_name()\n",
    "df_spells['dia_semana_fim'] = df_spells['data_fim'].dt.day_name()\n",
    "df_spells['mes'] = df_spells['data_inicio'].dt.month\n",
    "df_spells['ano'] = df_spells['data_inicio'].dt.year\n",
    "\n",
    "# Categorizar spells\n",
    "df_spells['categoria_spell'] = pd.cut(\n",
    "    df_spells['duracao_dias'],\n",
    "    bins=[0, 1, 3, 7, 14, float('inf')],\n",
    "    labels=['1 dia', '2-3 dias', '4-7 dias', '8-14 dias', '>14 dias']\n",
    ")\n",
    "\n",
    "df_spells['short_term'] = df_spells['duracao_dias'] <= 3\n",
    "df_spells['long_term'] = df_spells['duracao_dias'] > 14\n",
    "\n",
    "print(f'\\n‚úì Dataset de spells criado: {len(df_spells):,} spells')\n",
    "print(f'\\nDistribui√ß√£o por dura√ß√£o:')\n",
    "print(df_spells['categoria_spell'].value_counts().sort_index())\n",
    "\n",
    "print(f'\\nEstat√≠sticas de dura√ß√£o:')\n",
    "print(f'   M√©dia: {df_spells[\"duracao_dias\"].mean():.2f} dias')\n",
    "print(f'   Mediana: {df_spells[\"duracao_dias\"].median():.0f} dias')\n",
    "print(f'   P75: {df_spells[\"duracao_dias\"].quantile(0.75):.0f} dias')\n",
    "print(f'   P95: {df_spells[\"duracao_dias\"].quantile(0.95):.0f} dias')\n",
    "print(f'   M√°ximo: {df_spells[\"duracao_dias\"].max()} dias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3 An√°lise de spells por colaborador\n",
    "print('Analisando spells por colaborador...')\n",
    "\n",
    "df_colab_spells = df_spells.groupby('login_colaborador').agg({\n",
    "    'spell_id': 'count',  # Frequency rate (n√∫mero de spells)\n",
    "    'duracao_dias': ['sum', 'mean', 'median', 'std'],\n",
    "    'short_term': 'sum',  # N√∫mero de spells curtos\n",
    "    'long_term': 'sum',   # N√∫mero de spells longos\n",
    "}).reset_index()\n",
    "\n",
    "df_colab_spells.columns = [\n",
    "    'login_colaborador', 'num_spells', \n",
    "    'total_dias_ausentes', 'mean_spell_duration', 'median_spell_duration', 'std_spell_duration',\n",
    "    'num_short_term_spells', 'num_long_term_spells'\n",
    "]\n",
    "\n",
    "# Adicionar nome\n",
    "df_colab_spells = df_colab_spells.merge(\n",
    "    df[['login_colaborador', 'nome_colaborador']].drop_duplicates(),\n",
    "    on='login_colaborador'\n",
    ")\n",
    "\n",
    "print(f'\\n‚úì An√°lise por colaborador criada: {len(df_colab_spells):,} colaboradores')\n",
    "print(f'\\nTop 10 colaboradores por n√∫mero de spells:')\n",
    "print(df_colab_spells.nlargest(10, 'num_spells')[[\n",
    "    'nome_colaborador', 'num_spells', 'total_dias_ausentes', 'mean_spell_duration'\n",
    "]].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. M√âTRICAS CORE (KPIs ESSENCIAIS)\n",
    "\n",
    "Baseadas em frameworks de HR Analytics (AIHR, Fitzgerald HR)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 Calcular m√©tricas fundamentais\n",
    "print('=== M√âTRICAS CORE ===')\n",
    "\n",
    "# Per√≠odo de an√°lise\n",
    "data_inicio = df['Data'].min()\n",
    "data_fim = df['Data'].max()\n",
    "dias_calendario = (data_fim - data_inicio).days + 1\n",
    "num_colaboradores = df['login_colaborador'].nunique()\n",
    "\n",
    "# ‚ö†Ô∏è IMPORTANTE: Expandir listas de Nivel 1 e Nivel 2 para contagem correta\n",
    "df_expanded = df.copy()\n",
    "\n",
    "# Se Nivel 1 for lista, expandir\n",
    "if isinstance(df_expanded['Nivel 1'].iloc[0], list):\n",
    "    df_expanded = df_expanded.explode('Nivel 1')\n",
    "\n",
    "# Se Nivel 2 for lista, expandir\n",
    "if isinstance(df_expanded['Nivel 2'].iloc[0], list):\n",
    "    df_expanded = df_expanded.explode('Nivel 2')\n",
    "\n",
    "# Contar registos por tipo (agora vai funcionar!)\n",
    "num_presencas = df_expanded[df_expanded['Nivel 1'] == 'Trabalho Pago'].shape[0]\n",
    "num_atrasos = df_expanded[df_expanded['Nivel 1'] == 'Atraso'].shape[0]\n",
    "num_faltas = df_expanded[df_expanded['Nivel 1'].isin(['Falta Justificada', 'Falta Injustificada'])].shape[0]\n",
    "num_ausencias_medicas = df_expanded[df_expanded['Nivel 2'] == 'Aus√™ncia M√©dica'].shape[0]\n",
    "\n",
    "# KPI 1: Taxa de Absentismo Global\n",
    "# % Absentismo = Total de Faltas / (Presen√ßas + Total de Faltas)\n",
    "taxa_absentismo_global = (num_faltas / (num_presencas + num_faltas)) * 100\n",
    "\n",
    "# KPI 2: Lost Time Rate (dias perdidos por FTE)\n",
    "total_dias_perdidos = df_spells['duracao_dias'].sum()\n",
    "lost_time_rate = total_dias_perdidos / num_colaboradores\n",
    "\n",
    "# KPI 3: Frequency Rate (spells por colaborador)\n",
    "frequency_rate = len(df_spells) / num_colaboradores\n",
    "\n",
    "# KPI 4: Mean Spell Duration\n",
    "mean_spell_duration = df_spells['duracao_dias'].mean()\n",
    "\n",
    "# KPI 5: Taxa de Atrasos\n",
    "# % Atrasos = Atrasos / (Presen√ßas + Atrasos) - faz mais sentido\n",
    "taxa_atrasos = (num_atrasos / (num_presencas + num_atrasos)) * 100 if (num_presencas + num_atrasos) > 0 else 0\n",
    "\n",
    "# KPI 6: Taxa de Zero Aus√™ncias\n",
    "colaboradores_sem_ausencias = num_colaboradores - df_spells['login_colaborador'].nunique()\n",
    "taxa_zero_ausencias = (colaboradores_sem_ausencias / num_colaboradores) * 100\n",
    "\n",
    "# Exibir resultados\n",
    "print(f'\\nüìä PER√çODO DE AN√ÅLISE')\n",
    "print(f'   {data_inicio.date()} at√© {data_fim.date()} ({dias_calendario} dias)')\n",
    "print(f'   Colaboradores √∫nicos: {num_colaboradores:,}')\n",
    "print(f'\\nüìà M√âTRICAS PRINCIPAIS')\n",
    "print(f'   Presen√ßas: {num_presencas:,}')\n",
    "print(f'   Atrasos: {num_atrasos:,}')\n",
    "print(f'   Faltas (Just.+Injust.): {num_faltas:,}')\n",
    "print(f'   Aus√™ncias M√©dicas: {num_ausencias_medicas:,}')\n",
    "print(f'\\nüéØ KPIs')\n",
    "print(f'   Taxa de Absentismo: {taxa_absentismo_global:.2f}%')\n",
    "print(f'   Taxa de Atrasos: {taxa_atrasos:.2f}%')\n",
    "print(f'   Lost Time Rate: {lost_time_rate:.1f} dias/colaborador')\n",
    "print(f'   Frequency Rate: {frequency_rate:.2f} spells/colaborador')\n",
    "print(f'   Dura√ß√£o M√©dia Spell: {mean_spell_duration:.1f} dias')\n",
    "print(f'   Colaboradores sem aus√™ncias: {taxa_zero_ausencias:.1f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4B. AN√ÅLISE ESPEC√çFICA DE ATRASOS\n",
    "\n",
    "An√°lise dedicada aos atrasos (delays), separada das aus√™ncias.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4B.1 Criar dataset espec√≠fico para atrasos\n",
    "print('=== AN√ÅLISE ESPEC√çFICA DE ATRASOS ===\\n')\n",
    "\n",
    "# Expandir e filtrar atrasos\n",
    "df_atrasos = df.copy()\n",
    "\n",
    "# Expandir Nivel 1 se for lista\n",
    "if isinstance(df_atrasos['Nivel 1'].iloc[0], list):\n",
    "    df_atrasos = df_atrasos.explode('Nivel 1')\n",
    "\n",
    "df_atrasos = df_atrasos[df_atrasos['Nivel 1'] == 'Atraso'].copy()\n",
    "\n",
    "print(f'Total de registos de atraso: {len(df_atrasos):,}')\n",
    "\n",
    "if len(df_atrasos) > 0:\n",
    "    print(f'Colaboradores com atrasos: {df_atrasos[\"login_colaborador\"].nunique():,}')\n",
    "\n",
    "    # An√°lise por colaborador\n",
    "    atrasos_por_colab = df_atrasos.groupby('login_colaborador').agg({\n",
    "        'Data': 'count',\n",
    "        'nome_colaborador': 'first'\n",
    "    }).rename(columns={'Data': 'num_atrasos'}).sort_values('num_atrasos', ascending=False)\n",
    "\n",
    "    print(f'\\nüìä Estat√≠sticas:')\n",
    "    print(f'   M√©dia: {atrasos_por_colab[\"num_atrasos\"].mean():.1f} atrasos/colaborador')\n",
    "    print(f'   Mediana: {atrasos_por_colab[\"num_atrasos\"].median():.0f}')\n",
    "    print(f'   M√°ximo: {atrasos_por_colab[\"num_atrasos\"].max():.0f}')\n",
    "\n",
    "    # Top 10\n",
    "    print(f'\\nüîù TOP 10 COLABORADORES COM MAIS ATRASOS:\\n')\n",
    "    for idx, (login, row) in enumerate(atrasos_por_colab.head(10).iterrows(), 1):\n",
    "        print(f'{idx:2d}. {row[\"nome_colaborador\"][:40]:40s}: {row[\"num_atrasos\"]:3d} atrasos')\n",
    "\n",
    "    # Distribui√ß√£o\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Histogram(\n",
    "        x=atrasos_por_colab['num_atrasos'],\n",
    "        nbinsx=30,\n",
    "        marker_color='orange',\n",
    "        marker_line_color='white',\n",
    "        marker_line_width=1\n",
    "    ))\n",
    "    fig.update_layout(\n",
    "        title='Distribui√ß√£o de Atrasos por Colaborador',\n",
    "        xaxis_title='N√∫mero de Atrasos',\n",
    "        yaxis_title='Colaboradores',\n",
    "        height=400\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "    # Evolu√ß√£o temporal\n",
    "    atrasos_por_data = df_atrasos.groupby('Data').size().reset_index(name='num_atrasos')\n",
    "\n",
    "    fig2 = go.Figure()\n",
    "    fig2.add_trace(go.Scatter(\n",
    "        x=atrasos_por_data['Data'],\n",
    "        y=atrasos_por_data['num_atrasos'],\n",
    "        mode='lines',\n",
    "        line=dict(color='orange', width=2),\n",
    "        fill='tozeroy',\n",
    "        fillcolor='rgba(255,165,0,0.2)'\n",
    "    ))\n",
    "    fig2.update_layout(\n",
    "        title='Evolu√ß√£o Temporal de Atrasos',\n",
    "        xaxis_title='Data',\n",
    "        yaxis_title='N√∫mero de Atrasos',\n",
    "        height=400,\n",
    "        hovermode='x unified'\n",
    "    )\n",
    "    fig2.show()\n",
    "\n",
    "    # Exportar\n",
    "    atrasos_por_colab.to_excel('analise_atrasos.xlsx')\n",
    "    print('\\n‚úì Exportado: analise_atrasos.xlsx')\n",
    "else:\n",
    "    print('‚ö†Ô∏è  Nenhum atraso encontrado no dataset')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. BRADFORD FACTOR ANALYSIS\n",
    "\n",
    "**Bradford Factor** = S¬≤ √ó D\n",
    "- S = N√∫mero de spells (epis√≥dios)\n",
    "- D = Total de dias ausentes\n",
    "\n",
    "**Princ√≠pio**: Aus√™ncias curtas e frequentes s√£o **mais disruptivas** que aus√™ncias longas ocasionais.\n",
    "\n",
    "**Exemplo**:\n",
    "- Colaborador A: 1 spell de 10 dias ‚Üí B = 1¬≤ √ó 10 = **10**\n",
    "- Colaborador B: 10 spells de 1 dia ‚Üí B = 10¬≤ √ó 10 = **1000** (100x pior!)\n",
    "\n",
    "**Thresholds t√≠picos** (Call Centre Helper):\n",
    "- < 45: Aceit√°vel\n",
    "- 45-100: Conversa informal  \n",
    "- 100-200: Revis√£o formal\n",
    "- 200-500: Aviso escrito\n",
    "- 500-900: A√ß√£o disciplinar\n",
    "- \\> 900: Preocupa√ß√£o s√©ria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1 Calcular Bradford Factor por colaborador\n",
    "print('Calculando Bradford Factor...')\n",
    "\n",
    "df_bradford = df_colab_spells.copy()\n",
    "df_bradford['bradford_score'] = (df_bradford['num_spells'] ** 2) * df_bradford['total_dias_ausentes']\n",
    "\n",
    "# Categorizar por risk level\n",
    "def categorizar_bradford(score):\n",
    "    if score < 45:\n",
    "        return '1. Aceit√°vel (<45)'\n",
    "    elif score < 100:\n",
    "        return '2. Conversa Informal (45-100)'\n",
    "    elif score < 200:\n",
    "        return '3. Revis√£o Formal (100-200)'\n",
    "    elif score < 500:\n",
    "        return '4. Aviso Escrito (200-500)'\n",
    "    elif score < 900:\n",
    "        return '5. A√ß√£o Disciplinar (500-900)'\n",
    "    else:\n",
    "        return '6. Preocupa√ß√£o S√©ria (>900)'\n",
    "\n",
    "df_bradford['risk_level'] = df_bradford['bradford_score'].apply(categorizar_bradford)\n",
    "\n",
    "print(f'\\n‚úì Bradford Factor calculado para {len(df_bradford):,} colaboradores')\n",
    "print(f'\\nDistribui√ß√£o por Risk Level:')\n",
    "dist_risk = df_bradford['risk_level'].value_counts().sort_index()\n",
    "for level, count in dist_risk.items():\n",
    "    pct = count / len(df_bradford) * 100\n",
    "    print(f'   {level:40s}: {count:4,} ({pct:5.2f}%)')\n",
    "\n",
    "print(f'\\nEstat√≠sticas do Bradford Score:')\n",
    "print(f'   M√©dia: {df_bradford[\"bradford_score\"].mean():.2f}')\n",
    "print(f'   Mediana: {df_bradford[\"bradford_score\"].median():.2f}')\n",
    "print(f'   P75: {df_bradford[\"bradford_score\"].quantile(0.75):.2f}')\n",
    "print(f'   P90: {df_bradford[\"bradford_score\"].quantile(0.90):.2f}')\n",
    "print(f'   P95: {df_bradford[\"bradford_score\"].quantile(0.95):.2f}')\n",
    "print(f'   M√°ximo: {df_bradford[\"bradford_score\"].max():.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2 Top colaboradores para aten√ß√£o de RH\n",
    "print('\\nüî¥ TOP 20 COLABORADORES PARA ATEN√á√ÉO DE RH (Bradford Factor)')\n",
    "print('='*100)\n",
    "\n",
    "top20_bradford = df_bradford.nlargest(20, 'bradford_score')[\n",
    "    ['nome_colaborador', 'num_spells', 'total_dias_ausentes', \n",
    "     'mean_spell_duration', 'bradford_score', 'risk_level']\n",
    "].copy()\n",
    "\n",
    "top20_bradford.columns = ['Nome', 'N¬∫ Spells', 'Total Dias', 'Dura√ß√£o M√©dia', 'Bradford', 'Risk Level']\n",
    "\n",
    "print(top20_bradford.to_string(index=False))\n",
    "\n",
    "# Exportar para Excel\n",
    "top20_bradford.to_excel('bradford_factor_top20.xlsx', index=False)\n",
    "print('\\n‚úì Top 20 exportado para: bradford_factor_top20.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.3 Visualiza√ß√£o: Distribui√ß√£o de Bradford Factor\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=['Distribui√ß√£o por Risk Level', 'Scatter: Spells vs Dias (Bradford)']\n",
    ")\n",
    "\n",
    "# Gr√°fico 1: Barras de risk level\n",
    "dist_risk_sorted = dist_risk.sort_index()\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=[x.split('. ')[1].split(' (')[0] for x in dist_risk_sorted.index],\n",
    "        y=dist_risk_sorted.values,\n",
    "        marker_color=['green', 'yellow', 'orange', 'red', 'darkred', 'black'][:len(dist_risk_sorted)],\n",
    "        text=dist_risk_sorted.values,\n",
    "        textposition='auto'\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Gr√°fico 2: Scatter plot\n",
    "# Colorir por risk level\n",
    "color_map = {\n",
    "    '1. Aceit√°vel (<45)': 'green',\n",
    "    '2. Conversa Informal (45-100)': 'yellow',\n",
    "    '3. Revis√£o Formal (100-200)': 'orange',\n",
    "    '4. Aviso Escrito (200-500)': 'red',\n",
    "    '5. A√ß√£o Disciplinar (500-900)': 'darkred',\n",
    "    '6. Preocupa√ß√£o S√©ria (>900)': 'black'\n",
    "}\n",
    "\n",
    "for risk_level in df_bradford['risk_level'].unique():\n",
    "    df_temp = df_bradford[df_bradford['risk_level'] == risk_level]\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df_temp['num_spells'],\n",
    "            y=df_temp['total_dias_ausentes'],\n",
    "            mode='markers',\n",
    "            name=risk_level.split('. ')[1].split(' (')[0],\n",
    "            marker=dict(\n",
    "                size=8,\n",
    "                color=color_map.get(risk_level, 'gray'),\n",
    "                opacity=0.7\n",
    "            ),\n",
    "            text=df_temp['nome_colaborador'],\n",
    "            hovertemplate='<b>%{text}</b><br>Spells: %{x}<br>Dias: %{y}<extra></extra>'\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "\n",
    "fig.update_xaxes(title_text='Risk Level', row=1, col=1)\n",
    "fig.update_yaxes(title_text='N√∫mero de Colaboradores', row=1, col=1)\n",
    "fig.update_xaxes(title_text='N√∫mero de Spells', row=1, col=2)\n",
    "fig.update_yaxes(title_text='Total Dias Ausentes', row=1, col=2)\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Bradford Factor Analysis',\n",
    "    height=500,\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.4 Visualiza√ß√£o EXTRA: Bradford Heatmap por Opera√ß√£o e Categoria\n",
    "print('\\nCriando heatmap Bradford por Opera√ß√£o √ó Categoria...')\n",
    "\n",
    "if 'operacao' in df.columns:\n",
    "    # Merge Bradford scores com opera√ß√£o\n",
    "    df_bradford_op = df_bradford.merge(\n",
    "        df[['login_colaborador', 'operacao']].drop_duplicates(),\n",
    "        on='login_colaborador',\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # Merge com categoria profissional\n",
    "    df_bradford_op = df_bradford_op.merge(\n",
    "        df[['login_colaborador', 'categoria_profissional']].drop_duplicates(),\n",
    "        on='login_colaborador',\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # Top 10 opera√ß√µes\n",
    "    top_ops = df['operacao'].value_counts().head(10).index\n",
    "    df_bradford_op_filt = df_bradford_op[df_bradford_op['operacao'].isin(top_ops)]\n",
    "\n",
    "    # Top 5 categorias\n",
    "    top_cats = df['categoria_profissional'].value_counts().head(5).index\n",
    "    df_bradford_op_filt = df_bradford_op_filt[df_bradford_op_filt['categoria_profissional'].isin(top_cats)]\n",
    "\n",
    "    # Pivot: opera√ß√£o √ó categoria\n",
    "    pivot_bradford = df_bradford_op_filt.pivot_table(\n",
    "        index='operacao',\n",
    "        columns='categoria_profissional',\n",
    "        values='bradford_score',\n",
    "        aggfunc='mean'\n",
    "    ).fillna(0)\n",
    "\n",
    "    # Heatmap\n",
    "    fig = go.Figure(data=go.Heatmap(\n",
    "        z=pivot_bradford.values,\n",
    "        x=pivot_bradford.columns,\n",
    "        y=pivot_bradford.index,\n",
    "        colorscale='RdYlGn_r',  # Vermelho = alto, Verde = baixo\n",
    "        text=pivot_bradford.values.round(0),\n",
    "        texttemplate='%{text}',\n",
    "        textfont={\"size\": 10},\n",
    "        hovertemplate='Opera√ß√£o: %{y}<br>Categoria: %{x}<br>Bradford M√©dio: %{z:.0f}<extra></extra>'\n",
    "    ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title='Bradford Factor M√©dio: Opera√ß√£o √ó Categoria Profissional',\n",
    "        xaxis_title='Categoria Profissional',\n",
    "        yaxis_title='Opera√ß√£o',\n",
    "        height=600,\n",
    "        width=1000\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "    print('‚úì Heatmap criado')\n",
    "else:\n",
    "    print('‚ö†Ô∏è Campo \"operacao\" n√£o encontrado')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.5 Visualiza√ß√£o EXTRA: Funil de A√ß√£o (Bradford Thresholds)\n",
    "print('\\nCriando funil de a√ß√£o...')\n",
    "\n",
    "# Contar colaboradores por threshold\n",
    "thresholds = [\n",
    "    (0, 45, 'Aceit√°vel'),\n",
    "    (45, 100, 'Conversa Informal'),\n",
    "    (100, 200, 'Revis√£o Formal'),\n",
    "    (200, 500, 'Aviso Escrito'),\n",
    "    (500, 900, 'A√ß√£o Disciplinar'),\n",
    "    (900, float('inf'), 'Preocupa√ß√£o S√©ria')\n",
    "]\n",
    "\n",
    "funnel_data = []\n",
    "for min_val, max_val, label in thresholds:\n",
    "    count = ((df_bradford['bradford_score'] >= min_val) &\n",
    "             (df_bradford['bradford_score'] < max_val)).sum()\n",
    "    funnel_data.append({'N√≠vel': label, 'Colaboradores': count})\n",
    "\n",
    "df_funnel = pd.DataFrame(funnel_data)\n",
    "\n",
    "# Criar funil (inverted para mostrar prioridade)\n",
    "fig = go.Figure()\n",
    "\n",
    "colors = ['green', 'yellow', 'orange', 'red', 'darkred', 'black']\n",
    "\n",
    "fig.add_trace(go.Funnel(\n",
    "    y=df_funnel['N√≠vel'],\n",
    "    x=df_funnel['Colaboradores'],\n",
    "    textposition='inside',\n",
    "    textinfo='value+percent initial',\n",
    "    marker=dict(color=colors),\n",
    "    connector={\"line\": {\"color\": \"gray\", \"dash\": \"dot\", \"width\": 2}}\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Funil de A√ß√£o: Colaboradores por N√≠vel de Risco (Bradford Factor)',\n",
    "    height=500,\n",
    "    width=800\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print('\\nüìä FUNIL DE A√á√ÉO')\n",
    "for _, row in df_funnel.iterrows():\n",
    "    print(f'   {row[\"N√≠vel\"]:30s}: {row[\"Colaboradores\"]:4,} colaboradores')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.6 Visualiza√ß√£o EXTRA: Timeline de Spells (Frequ√™ncia vs Dura√ß√£o)\n",
    "print('\\nCriando timeline de spells...')\n",
    "\n",
    "# Agrupar spells por m√™s\n",
    "df_spells['mes_inicio'] = df_spells['data_inicio'].dt.to_period('M').astype(str)\n",
    "\n",
    "spells_mensal = df_spells.groupby('mes_inicio').agg({\n",
    "    'spell_id': 'count',\n",
    "    'duracao_dias': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "spells_mensal.columns = ['Mes', 'Num_Spells', 'Duracao_Media']\n",
    "\n",
    "# Dual-axis plot\n",
    "fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=spells_mensal['Mes'],\n",
    "        y=spells_mensal['Num_Spells'],\n",
    "        name='N√∫mero de Spells',\n",
    "        marker_color='lightblue'\n",
    "    ),\n",
    "    secondary_y=False\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=spells_mensal['Mes'],\n",
    "        y=spells_mensal['Duracao_Media'],\n",
    "        name='Dura√ß√£o M√©dia (dias)',\n",
    "        mode='lines+markers',\n",
    "        line=dict(color='red', width=3),\n",
    "        marker=dict(size=8)\n",
    "    ),\n",
    "    secondary_y=True\n",
    ")\n",
    "\n",
    "fig.update_xaxes(title_text='M√™s')\n",
    "fig.update_yaxes(title_text='N√∫mero de Spells', secondary_y=False)\n",
    "fig.update_yaxes(title_text='Dura√ß√£o M√©dia (dias)', secondary_y=True)\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Evolu√ß√£o Temporal: Frequ√™ncia vs Dura√ß√£o de Spells',\n",
    "    height=500,\n",
    "    hovermode='x unified'\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "print('‚úì Timeline criada')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**CONTINUA NA PR√ìXIMA C√âLULA**:\n",
    "- Dete√ß√£o de Padr√µes Suspeitos\n",
    "- An√°lise de Cohorts\n",
    "- Clustering\n",
    "- Network Analysis\n",
    "- Event Detection\n",
    "- Visualiza√ß√µes Avan√ßadas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. DETE√á√ÉO DE PADR√ïES SUSPEITOS üîç\n",
    "\n",
    "**Red Flags** para poss√≠vel abuso ou comportamento an√≥malo:\n",
    "- **Weekend Pattern**: Faltas consistentes √†s sextas/segundas\n",
    "- **Bridge Pattern**: Aus√™ncias adjacentes a feriados  \n",
    "- **Temporal Consistency**: Sempre no mesmo dia/semana/m√™s\n",
    "- **Fragmenta√ß√£o**: Bradford score alto com poucos dias totais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.1 Padr√£o Segunda/Sexta - Spells que come√ßam/terminam em fim de semana\n",
    "print('Analisando padr√µes de segunda/sexta...')\n",
    "\n",
    "# Propor√ß√£o de spells que come√ßam √† segunda\n",
    "spells_inicio_segunda = df_spells[df_spells['dia_semana_inicio'] == 'Monday'].shape[0]\n",
    "prop_inicio_segunda = spells_inicio_segunda / len(df_spells) * 100\n",
    "\n",
    "# Propor√ß√£o de spells que terminam √† sexta\n",
    "spells_fim_sexta = df_spells[df_spells['dia_semana_fim'] == 'Friday'].shape[0]\n",
    "prop_fim_sexta = spells_fim_sexta / len(df_spells) * 100\n",
    "\n",
    "# Spells que combinam ambos (ponte de fim de semana)\n",
    "spells_ponte_fds = df_spells[\n",
    "    (df_spells['dia_semana_inicio'] == 'Monday') & \n",
    "    (df_spells['dia_semana_fim'] == 'Friday')\n",
    "].shape[0]\n",
    "\n",
    "print(f'\\nüìä PADR√ÉO SEGUNDA/SEXTA')\n",
    "print(f'   Spells que come√ßam √† segunda: {spells_inicio_segunda:,} ({prop_inicio_segunda:.2f}%)')\n",
    "print(f'   Spells que terminam √† sexta: {spells_fim_sexta:,} ({prop_fim_sexta:.2f}%)')\n",
    "print(f'   Spells \"ponte de semana\" (2¬™ ‚Üí 6¬™): {spells_ponte_fds:,}')\n",
    "\n",
    "# An√°lise por colaborador\n",
    "df_padroes_colab = df_spells.groupby('login_colaborador').agg({\n",
    "    'spell_id': 'count',\n",
    "    'dia_semana_inicio': lambda x: (x == 'Monday').sum(),\n",
    "    'dia_semana_fim': lambda x: (x == 'Friday').sum()\n",
    "}).reset_index()\n",
    "\n",
    "df_padroes_colab.columns = ['login_colaborador', 'total_spells', 'inicio_segunda', 'fim_sexta']\n",
    "\n",
    "# Calcular propor√ß√µes\n",
    "df_padroes_colab['prop_inicio_segunda'] = df_padroes_colab['inicio_segunda'] / df_padroes_colab['total_spells']\n",
    "df_padroes_colab['prop_fim_sexta'] = df_padroes_colab['fim_sexta'] / df_padroes_colab['total_spells']\n",
    "\n",
    "# Flag: > 50% dos spells come√ßam √† segunda OU terminam √† sexta\n",
    "df_padroes_colab['flag_weekend_pattern'] = (\n",
    "    (df_padroes_colab['prop_inicio_segunda'] > 0.5) | \n",
    "    (df_padroes_colab['prop_fim_sexta'] > 0.5)\n",
    ") & (df_padroes_colab['total_spells'] >= 3)  # M√≠nimo 3 spells para ser significativo\n",
    "\n",
    "# Adicionar nomes\n",
    "df_padroes_colab = df_padroes_colab.merge(\n",
    "    df[['login_colaborador', 'nome_colaborador']].drop_duplicates(),\n",
    "    on='login_colaborador'\n",
    ")\n",
    "\n",
    "n_flagged = df_padroes_colab['flag_weekend_pattern'].sum()\n",
    "print(f'\\nüö© COLABORADORES COM PADR√ÉO SUSPEITO: {n_flagged}')\n",
    "print(f'   (>50% spells come√ßam 2¬™ OU terminam 6¬™, com m√≠nimo 3 spells)')\n",
    "\n",
    "if n_flagged > 0:\n",
    "    print(f'\\nTop 10:')\n",
    "    top_weekend = df_padroes_colab[df_padroes_colab['flag_weekend_pattern']].nlargest(10, 'total_spells')[[\n",
    "        'nome_colaborador', 'total_spells', 'inicio_segunda', 'fim_sexta', \n",
    "        'prop_inicio_segunda', 'prop_fim_sexta'\n",
    "    ]].copy()\n",
    "    top_weekend['prop_inicio_segunda'] = top_weekend['prop_inicio_segunda'].apply(lambda x: f'{x*100:.1f}%')\n",
    "    top_weekend['prop_fim_sexta'] = top_weekend['prop_fim_sexta'].apply(lambda x: f'{x*100:.1f}%')\n",
    "    print(top_weekend.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.2 Padr√µes de Ponte - Aus√™ncias adjacentes a feriados\n",
    "print('\\nAnalisando padr√µes de ponte (adjacentes a feriados)...')\n",
    "\n",
    "# NOTA: Precisamos de uma lista de feriados\n",
    "# Para demonstra√ß√£o, vamos criar feriados t√≠picos portugueses\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Feriados fixos comuns (exemplo para um ano)\n",
    "ano_min = df['Data'].min().year\n",
    "ano_max = df['Data'].max().year\n",
    "\n",
    "feriados = []\n",
    "for ano in range(ano_min, ano_max + 1):\n",
    "    feriados.extend([\n",
    "        datetime(ano, 1, 1),   # Ano Novo\n",
    "        datetime(ano, 4, 25),  # 25 de Abril\n",
    "        datetime(ano, 5, 1),   # Dia do Trabalhador\n",
    "        datetime(ano, 6, 10),  # Dia de Portugal\n",
    "        datetime(ano, 8, 15),  # Assun√ß√£o\n",
    "        datetime(ano, 10, 5),  # Implanta√ß√£o da Rep√∫blica\n",
    "        datetime(ano, 11, 1),  # Todos os Santos\n",
    "        datetime(ano, 12, 1),  # Restaura√ß√£o da Independ√™ncia\n",
    "        datetime(ano, 12, 8),  # Imaculada Concei√ß√£o\n",
    "        datetime(ano, 12, 25), # Natal\n",
    "    ])\n",
    "\n",
    "feriados = pd.to_datetime(feriados)\n",
    "\n",
    "print(f'   Feriados considerados: {len(feriados)}')\n",
    "\n",
    "# Identificar spells que come√ßam/terminam adjacentes a feriados\n",
    "def is_adjacent_to_holiday(date, holidays, tolerance=1):\n",
    "    \"\"\"Verifica se data est√° a ¬±tolerance dias de um feriado\"\"\"\n",
    "    for holiday in holidays:\n",
    "        diff = abs((date - holiday).days)\n",
    "        if diff <= tolerance:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "df_spells['inicio_adjacente_feriado'] = df_spells['data_inicio'].apply(\n",
    "    lambda x: is_adjacent_to_holiday(x, feriados)\n",
    ")\n",
    "\n",
    "df_spells['fim_adjacente_feriado'] = df_spells['data_fim'].apply(\n",
    "    lambda x: is_adjacent_to_holiday(x, feriados)\n",
    ")\n",
    "\n",
    "n_inicio_adj = df_spells['inicio_adjacente_feriado'].sum()\n",
    "n_fim_adj = df_spells['fim_adjacente_feriado'].sum()\n",
    "\n",
    "print(f'\\nüìä PADR√ïES DE PONTE')\n",
    "print(f'   Spells que come√ßam adjacentes a feriado: {n_inicio_adj:,} ({n_inicio_adj/len(df_spells)*100:.2f}%)')\n",
    "print(f'   Spells que terminam adjacentes a feriado: {n_fim_adj:,} ({n_fim_adj/len(df_spells)*100:.2f}%)')\n",
    "\n",
    "# Por colaborador\n",
    "df_ponte_colab = df_spells.groupby('login_colaborador').agg({\n",
    "    'spell_id': 'count',\n",
    "    'inicio_adjacente_feriado': 'sum',\n",
    "    'fim_adjacente_feriado': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "df_ponte_colab.columns = ['login_colaborador', 'total_spells', 'inicio_adj_feriado', 'fim_adj_feriado']\n",
    "df_ponte_colab['prop_ponte'] = (\n",
    "    (df_ponte_colab['inicio_adj_feriado'] + df_ponte_colab['fim_adj_feriado']) / \n",
    "    (df_ponte_colab['total_spells'] * 2)\n",
    ")\n",
    "\n",
    "# Flag: > 40% adjacentes a feriados\n",
    "df_ponte_colab['flag_bridge_pattern'] = (\n",
    "    (df_ponte_colab['prop_ponte'] > 0.4) & \n",
    "    (df_ponte_colab['total_spells'] >= 3)\n",
    ")\n",
    "\n",
    "df_ponte_colab = df_ponte_colab.merge(\n",
    "    df[['login_colaborador', 'nome_colaborador']].drop_duplicates(),\n",
    "    on='login_colaborador'\n",
    ")\n",
    "\n",
    "n_flagged_ponte = df_ponte_colab['flag_bridge_pattern'].sum()\n",
    "print(f'\\nüö© COLABORADORES COM PADR√ÉO DE PONTE SUSPEITO: {n_flagged_ponte}')\n",
    "\n",
    "if n_flagged_ponte > 0:\n",
    "    print(f'\\nTop 10:')\n",
    "    top_ponte = df_ponte_colab[df_ponte_colab['flag_bridge_pattern']].nlargest(10, 'total_spells')[[\n",
    "        'nome_colaborador', 'total_spells', 'inicio_adj_feriado', 'fim_adj_feriado'\n",
    "    ]]\n",
    "    print(top_ponte.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.3 Outlier Detection - Colaboradores estatisticamente fora do padr√£o\n",
    "print('\\nDete√ß√£o de outliers estat√≠sticos...')\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "# Z-score para v√°rias m√©tricas\n",
    "df_outliers = df_bradford.copy()\n",
    "\n",
    "# Calcular Z-scores\n",
    "df_outliers['z_num_spells'] = stats.zscore(df_outliers['num_spells'])\n",
    "df_outliers['z_total_dias'] = stats.zscore(df_outliers['total_dias_ausentes'])\n",
    "df_outliers['z_bradford'] = stats.zscore(df_outliers['bradford_score'])\n",
    "\n",
    "# Outlier se Z-score > 3 em qualquer m√©trica\n",
    "df_outliers['is_outlier'] = (\n",
    "    (abs(df_outliers['z_num_spells']) > 3) |\n",
    "    (abs(df_outliers['z_total_dias']) > 3) |\n",
    "    (abs(df_outliers['z_bradford']) > 3)\n",
    ")\n",
    "\n",
    "n_outliers = df_outliers['is_outlier'].sum()\n",
    "print(f'\\nüéØ OUTLIERS DETETADOS: {n_outliers}')\n",
    "print(f'   ({n_outliers / len(df_outliers) * 100:.2f}% dos colaboradores)')\n",
    "\n",
    "if n_outliers > 0:\n",
    "    print(f'\\nOutliers (Z-score > 3 em alguma m√©trica):')\n",
    "    outliers = df_outliers[df_outliers['is_outlier']][\n",
    "        ['nome_colaborador', 'num_spells', 'total_dias_ausentes', 'bradford_score',\n",
    "         'z_num_spells', 'z_total_dias', 'z_bradford']\n",
    "    ].nlargest(15, 'z_bradford')\n",
    "    \n",
    "    for col in ['z_num_spells', 'z_total_dias', 'z_bradford']:\n",
    "        outliers[col] = outliers[col].apply(lambda x: f'{x:.2f}')\n",
    "    \n",
    "    print(outliers.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. AN√ÅLISE DE COHORTS (por Data de Ingresso)\n",
    "\n",
    "**Objetivo**: Investigar se colaboradores mais novos t√™m taxas de absentismo diferentes.\n",
    "\n",
    "**Hip√≥teses a testar**:\n",
    "- Novos colaboradores podem ter mais aus√™ncias (adapta√ß√£o, problemas iniciais)\n",
    "- Ou menos aus√™ncias (honeymoon period, medo de consequ√™ncias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.1 An√°lise por Cohort (baseado em DtActivacao REAL)\n",
    "print('=== AN√ÅLISE POR COHORT (SENIORIDADE) ===\\n')\n",
    "\n",
    "if 'DtActivacao' not in df.columns:\n",
    "    print('‚ùå Campo \"DtActivacao\" n√£o encontrado!')\n",
    "    print('   An√°lise de cohorts requer data de ativa√ß√£o real.')\n",
    "    print('   Pulando esta se√ß√£o.')\n",
    "else:\n",
    "    print('‚úì Campo DtActivacao encontrado')\n",
    "\n",
    "    # Preparar dados\n",
    "    df_cohort = df[['login_colaborador', 'nome_colaborador', 'DtActivacao', 'Data', 'Nivel 1']].copy()\n",
    "    df_cohort['DtActivacao'] = pd.to_datetime(df_cohort['DtActivacao'], errors='coerce')\n",
    "    df_cohort = df_cohort.dropna(subset=['DtActivacao'])\n",
    "\n",
    "    print(f'Colaboradores com DtActivacao v√°lida: {df_cohort[\"login_colaborador\"].nunique():,}')\n",
    "\n",
    "    # Calcular senioridade\n",
    "    data_ref = df_cohort['Data'].max()\n",
    "    df_cohort['senioridade_anos'] = (data_ref - df_cohort['DtActivacao']).dt.days / 365.25\n",
    "\n",
    "    # Criar cohorts\n",
    "    df_cohort['cohort'] = pd.cut(\n",
    "        df_cohort['senioridade_anos'],\n",
    "        bins=[0, 1, 2, 3, 5, 100],\n",
    "        labels=['<1 ano', '1-2 anos', '2-3 anos', '3-5 anos', '>5 anos']\n",
    "    )\n",
    "\n",
    "    # Expandir e filtrar aus√™ncias\n",
    "    df_cohort_exp = df_cohort.copy()\n",
    "    if isinstance(df_cohort_exp['Nivel 1'].iloc[0], list):\n",
    "        df_cohort_exp = df_cohort_exp.explode('Nivel 1')\n",
    "\n",
    "    df_cohort_abs = df_cohort_exp[\n",
    "        df_cohort_exp['Nivel 1'].isin(['Falta Justificada', 'Falta Injustificada', 'Aus√™ncia'])\n",
    "    ]\n",
    "\n",
    "    # Estat√≠sticas\n",
    "    cohort_stats = df_cohort_abs.groupby('cohort').agg({\n",
    "        'login_colaborador': 'nunique',\n",
    "        'Data': 'count'\n",
    "    }).rename(columns={'login_colaborador': 'num_colaboradores', 'Data': 'total_ausencias'})\n",
    "\n",
    "    cohort_stats['media_ausencias'] = cohort_stats['total_ausencias'] / cohort_stats['num_colaboradores']\n",
    "\n",
    "    print('\\nüìä Absentismo por Cohort:')\n",
    "    print(cohort_stats)\n",
    "\n",
    "    # Visualiza√ß√£o\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Bar(\n",
    "        x=cohort_stats.index,\n",
    "        y=cohort_stats['media_ausencias'],\n",
    "        marker_color='skyblue'\n",
    "    ))\n",
    "    fig.update_layout(\n",
    "        title='M√©dia de Aus√™ncias por Cohort de Senioridade',\n",
    "        xaxis_title='Cohort',\n",
    "        yaxis_title='M√©dia de Aus√™ncias',\n",
    "        height=400\n",
    "    )\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. CLUSTERING DE PERFIS DE ABSENTISMO\n",
    "\n",
    "**Objetivo**: Segmentar colaboradores em grupos homog√©neos por comportamento de absentismo.\n",
    "\n",
    "**Features para clustering**:\n",
    "- N√∫mero de spells (frequ√™ncia)\n",
    "- Total de dias ausentes\n",
    "- Dura√ß√£o m√©dia de spell\n",
    "- Bradford score\n",
    "- Propor√ß√£o de short-term spells\n",
    "\n",
    "**Algoritmo**: K-Means (3-5 clusters t√≠picos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.1 Preparar dados para clustering\n",
    "print('Preparando features para clustering...')\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Selecionar features\n",
    "features_clustering = [\n",
    "    'num_spells',\n",
    "    'total_dias_ausentes', \n",
    "    'mean_spell_duration',\n",
    "    'bradford_score',\n",
    "    'num_short_term_spells'\n",
    "]\n",
    "\n",
    "df_cluster = df_bradford[['login_colaborador', 'nome_colaborador'] + features_clustering].copy()\n",
    "\n",
    "# Normalizar features (importante para K-Means)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(df_cluster[features_clustering])\n",
    "\n",
    "print(f'\\n‚úì Features preparadas: {X_scaled.shape}')\n",
    "\n",
    "# Determinar n√∫mero ideal de clusters (Elbow Method)\n",
    "print('\\nCalculando Elbow Method (K=2 a K=8)...')\n",
    "inertias = []\n",
    "K_range = range(2, 9)\n",
    "\n",
    "for k in K_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(X_scaled)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "\n",
    "# Visualizar Elbow\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=list(K_range),\n",
    "    y=inertias,\n",
    "    mode='lines+markers',\n",
    "    marker=dict(size=10)\n",
    "))\n",
    "fig.update_layout(\n",
    "    title='Elbow Method - Determinar K Ideal',\n",
    "    xaxis_title='N√∫mero de Clusters (K)',\n",
    "    yaxis_title='Inertia (Within-Cluster Sum of Squares)',\n",
    "    height=400\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# Aplicar K-Means com K=4 (t√≠pico para absentismo: baixo/moderado/alto/muito alto)\n",
    "K_optimal = 4\n",
    "print(f'\\nAplicando K-Means com K={K_optimal}...')\n",
    "\n",
    "kmeans = KMeans(n_clusters=K_optimal, random_state=42, n_init=10)\n",
    "df_cluster['cluster'] = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "print(f'\\n‚úì Clusters atribu√≠dos')\n",
    "print(f'\\nDistribui√ß√£o por cluster:')\n",
    "print(df_cluster['cluster'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.2 Valida√ß√£o com Silhouette Score\n",
    "print('\\nValidando n√∫mero de clusters com Silhouette Score...\\n')\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "silhouette_scores = []\n",
    "K_range = range(2, 11)\n",
    "\n",
    "for k in K_range:\n",
    "    kmeans_test = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    labels_test = kmeans_test.fit_predict(X_scaled)\n",
    "    score = silhouette_score(X_scaled, labels_test)\n",
    "    silhouette_scores.append(score)\n",
    "    print(f'K={k}: Silhouette Score = {score:.3f}')\n",
    "\n",
    "# Visualizar\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=list(K_range),\n",
    "    y=silhouette_scores,\n",
    "    mode='lines+markers',\n",
    "    marker=dict(size=10, color='blue'),\n",
    "    line=dict(width=2)\n",
    "))\n",
    "fig.update_layout(\n",
    "    title='Silhouette Score por N√∫mero de Clusters',\n",
    "    xaxis_title='K (N√∫mero de Clusters)',\n",
    "    yaxis_title='Silhouette Score',\n",
    "    height=400\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "best_k = list(K_range)[silhouette_scores.index(max(silhouette_scores))]\n",
    "print(f'\\nüí° Melhor K por Silhouette: {best_k} (score={max(silhouette_scores):.3f})')\n",
    "\n",
    "# Usar o melhor K\n",
    "K_optimal = best_k\n",
    "print(f'   Usando K={K_optimal} para clustering final')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.2 Interpretar clusters - Calcular m√©dias por cluster\n",
    "print('\\n=== CARACTERIZA√á√ÉO DOS CLUSTERS ===')\n",
    "\n",
    "cluster_profiles = df_cluster.groupby('cluster')[features_clustering].mean()\n",
    "\n",
    "# Ordenar por bradford_score (do menor ao maior)\n",
    "cluster_profiles = cluster_profiles.sort_values('bradford_score')\n",
    "\n",
    "# Renomear clusters de forma interpret√°vel\n",
    "cluster_mapping = {}\n",
    "for idx, (cluster_id, row) in enumerate(cluster_profiles.iterrows()):\n",
    "    if idx == 0:\n",
    "        cluster_mapping[cluster_id] = '1. BAIXO Absentismo'\n",
    "    elif idx == 1:\n",
    "        cluster_mapping[cluster_id] = '2. MODERADO Absentismo'\n",
    "    elif idx == 2:\n",
    "        cluster_mapping[cluster_id] = '3. ALTO Absentismo'\n",
    "    else:\n",
    "        cluster_mapping[cluster_id] = '4. MUITO ALTO Absentismo'\n",
    "\n",
    "df_cluster['cluster_nome'] = df_cluster['cluster'].map(cluster_mapping)\n",
    "\n",
    "# Exibir perfis\n",
    "print('\\nPERFIL M√âDIO POR CLUSTER:\\n')\n",
    "for cluster_id, row in cluster_profiles.iterrows():\n",
    "    cluster_nome = cluster_mapping[cluster_id]\n",
    "    n_colab = (df_cluster['cluster'] == cluster_id).sum()\n",
    "    \n",
    "    print(f'{cluster_nome} (n={n_colab})')\n",
    "    print(f'   N¬∫ Spells: {row[\"num_spells\"]:.2f}')\n",
    "    print(f'   Total Dias Ausentes: {row[\"total_dias_ausentes\"]:.2f}')\n",
    "    print(f'   Dura√ß√£o M√©dia Spell: {row[\"mean_spell_duration\"]:.2f} dias')\n",
    "    print(f'   Bradford Score: {row[\"bradford_score\"]:.2f}')\n",
    "    print(f'   Short-term Spells: {row[\"num_short_term_spells\"]:.2f}')\n",
    "    print()\n",
    "\n",
    "# Exportar colaboradores com clusters\n",
    "df_cluster_export = df_cluster[[\n",
    "    'nome_colaborador', 'cluster_nome', 'num_spells', 'total_dias_ausentes',\n",
    "    'mean_spell_duration', 'bradford_score'\n",
    "]].sort_values(['cluster_nome', 'bradford_score'], ascending=[True, False])\n",
    "\n",
    "df_cluster_export.to_excel('colaboradores_clusters_absentismo.xlsx', index=False)\n",
    "print('‚úì Clusters exportados para: colaboradores_clusters_absentismo.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.3 Visualizar clusters em 2D (PCA)\n",
    "print('\\nVisualizando clusters em 2D (PCA)...')\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Reduzir para 2 dimens√µes\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "df_cluster['PC1'] = X_pca[:, 0]\n",
    "df_cluster['PC2'] = X_pca[:, 1]\n",
    "\n",
    "print(f'Vari√¢ncia explicada: PC1={pca.explained_variance_ratio_[0]*100:.2f}%, PC2={pca.explained_variance_ratio_[1]*100:.2f}%')\n",
    "\n",
    "# Scatter plot\n",
    "fig = go.Figure()\n",
    "\n",
    "for cluster_nome in sorted(df_cluster['cluster_nome'].unique()):\n",
    "    df_temp = df_cluster[df_cluster['cluster_nome'] == cluster_nome]\n",
    "    \n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=df_temp['PC1'],\n",
    "        y=df_temp['PC2'],\n",
    "        mode='markers',\n",
    "        name=cluster_nome,\n",
    "        text=df_temp['nome_colaborador'],\n",
    "        marker=dict(size=8, opacity=0.7),\n",
    "        hovertemplate='<b>%{text}</b><br>PC1: %{x:.2f}<br>PC2: %{y:.2f}<extra></extra>'\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Clusters de Absentismo (PCA 2D)',\n",
    "    xaxis_title=f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}% var)',\n",
    "    yaxis_title=f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}% var)',\n",
    "    height=600\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. NETWORK ANALYSIS (Coincid√™ncias de Aus√™ncias)\n",
    "\n",
    "**Objetivo**: Detetar padr√µes de aus√™ncias **simult√¢neas** que possam indicar:\n",
    "- Eventos locais (gripe, problema na opera√ß√£o)\n",
    "- Problemas de equipa\n",
    "- Coincid√™ncias suspeitas\n",
    "\n",
    "**M√©todo**: Identificar dias com m√∫ltiplos colaboradores ausentes na mesma opera√ß√£o."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.1 Preparar dados: APENAS colaboradores ativos\n",
    "print('=== NETWORK ANALYSIS: CO-AUS√äNCIAS ===\\n')\n",
    "\n",
    "# Filtrar apenas aus√™ncias de colaboradores ativos\n",
    "if 'Activo?' in df.columns:\n",
    "    df_network = df[\n",
    "        (df['Nivel 1'].isin(['Falta Justificada', 'Falta Injustificada', 'Aus√™ncia'])) &\n",
    "        (df['Activo?'].isin(['Sim', True, 'sim', 'S']))\n",
    "    ].copy()\n",
    "    print(f'‚úì Filtrado para colaboradores ativos')\n",
    "else:\n",
    "    df_network = df[\n",
    "        df['Nivel 1'].isin(['Falta Justificada', 'Falta Injustificada', 'Aus√™ncia'])\n",
    "    ].copy()\n",
    "    print(f'‚ö†Ô∏è  Campo Activo? n√£o encontrado - usando todos')\n",
    "\n",
    "print(f'Registos: {len(df_network):,}')\n",
    "print(f'Colaboradores: {df_network[\"login_colaborador\"].nunique():,}')\n",
    "print(f'Per√≠odo: {df_network[\"Data\"].min().date()} at√© {df_network[\"Data\"].max().date()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.2 Calcular co-aus√™ncias e m√©tricas de overlap\n",
    "print('\\nCalculando co-aus√™ncias por pares...\\n')\n",
    "\n",
    "# Criar dicion√°rio: colaborador -> set de dias\n",
    "print('   Criando dicion√°rio de dias por colaborador...')\n",
    "colab_dias = {}\n",
    "for colab in df_network['login_colaborador'].unique():\n",
    "    dias_set = set(df_network[df_network['login_colaborador'] == colab]['Data'])\n",
    "    colab_dias[colab] = dias_set\n",
    "\n",
    "colaboradores = list(colab_dias.keys())\n",
    "print(f'   Colaboradores: {len(colaboradores):,}')\n",
    "\n",
    "# Calcular overlaps entre todos os pares\n",
    "print('   Calculando overlaps entre pares...')\n",
    "pares = []\n",
    "\n",
    "for i, colab_i in enumerate(colaboradores):\n",
    "    if i % 200 == 0:\n",
    "        print(f'      Processado {i}/{len(colaboradores)}...')\n",
    "\n",
    "    dias_i = colab_dias[colab_i]\n",
    "\n",
    "    for colab_j in colaboradores[i+1:]:\n",
    "        dias_j = colab_dias[colab_j]\n",
    "\n",
    "        # Co-aus√™ncias = interse√ß√£o\n",
    "        cooccur = len(dias_i & dias_j)\n",
    "\n",
    "        if cooccur >= 3:  # M√≠nimo 3 dias juntos\n",
    "            # Overlap em rela√ß√£o ao menor\n",
    "            # Jaccard Index = interse√ß√£o / uni√£o\n",
    "        union_size = len(dias_i | dias_j)\n",
    "        jaccard = cooccur / union_size if union_size > 0 else 0\n",
    "\n",
    "            pares.append({\n",
    "                'colab_i': colab_i,\n",
    "                'colab_j': colab_j,\n",
    "                'cooccur': cooccur,\n",
    "                'dias_i': len(dias_i),\n",
    "                'dias_j': len(dias_j),\n",
    "                'jaccard': jaccard\n",
    "            })\n",
    "\n",
    "df_pares = pd.DataFrame(pares)\n",
    "\n",
    "print(f'\\n‚úì Encontrados {len(df_pares):,} pares com ‚â•3 co-aus√™ncias')\n",
    "\n",
    "# Adicionar nomes\n",
    "df_pares = df_pares.merge(\n",
    "    df[['login_colaborador', 'nome_colaborador']].drop_duplicates(),\n",
    "    left_on='colab_i', right_on='login_colaborador'\n",
    ").rename(columns={'nome_colaborador': 'nome_i'}).drop('login_colaborador', axis=1)\n",
    "\n",
    "df_pares = df_pares.merge(\n",
    "    df[['login_colaborador', 'nome_colaborador']].drop_duplicates(),\n",
    "    left_on='colab_j', right_on='login_colaborador'\n",
    ").rename(columns={'nome_colaborador': 'nome_j'}).drop('login_colaborador', axis=1)\n",
    "\n",
    "# Ordenar por Jaccard Index\n",
    "df_pares = df_pares.sort_values('overlap_pct', ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.3 An√°lise de distribui√ß√£o para escolher threshold\n",
    "print('\\nAN√ÅLISE DE DISTRIBUI√á√ÉO:\\n')\n",
    "\n",
    "print(f'üìä Estat√≠sticas de Overlap:')\n",
    "print(f'   M√©dia: {df_pares[\"overlap_pct\"].mean()*100:.1f}%')\n",
    "print(f'   Mediana: {df_pares[\"overlap_pct\"].median()*100:.1f}%')\n",
    "print(f'   P75: {df_pares[\"overlap_pct\"].quantile(0.75)*100:.1f}%')\n",
    "print(f'   P90: {df_pares[\"overlap_pct\"].quantile(0.90)*100:.1f}%')\n",
    "print(f'   P95: {df_pares[\"overlap_pct\"].quantile(0.95)*100:.1f}%')\n",
    "print(f'   M√°ximo: {df_pares[\"overlap_pct\"].max()*100:.1f}%')\n",
    "\n",
    "# Histograma\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Histogram(\n",
    "    x=df_pares['overlap_pct'] * 100,\n",
    "    nbinsx=50,\n",
    "    marker_color='lightblue',\n",
    "    marker_line_color='white',\n",
    "    marker_line_width=1\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Distribui√ß√£o de Overlap % entre Pares',\n",
    "    xaxis_title='Overlap %',\n",
    "    yaxis_title='N√∫mero de Pares',\n",
    "    height=400\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Escolher threshold (P90)\n",
    "threshold = df_pares['overlap_pct'].quantile(0.90)\n",
    "print(f'\\nüí° Threshold escolhido (P90): {threshold*100:.1f}%')\n",
    "\n",
    "df_pares_sig = df_pares[df_pares['overlap_pct'] >= threshold].copy()\n",
    "print(f'   Pares significativos: {len(df_pares_sig):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.4 Top pares com maior overlap\n",
    "print('\\nüîù TOP 20 PARES POR OVERLAP:\\n')\n",
    "\n",
    "for idx, row in df_pares_sig.head(20).iterrows():\n",
    "    print(f\"{idx+1:2d}. {row['nome_i'][:30]:30s} + {row['nome_j'][:30]:30s}\")\n",
    "    print(f\"    Co-aus√™ncias: {row['cooccur']:3d} | Total i: {row['dias_i']:3d} | Total j: {row['dias_j']:3d} | Overlap: {row['overlap_pct']*100:5.1f}%\")\n",
    "\n",
    "# Exportar\n",
    "df_pares_sig.to_excel('network_pares_significativos.xlsx', index=False)\n",
    "print('\\n‚úì Exportado: network_pares_significativos.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.5 Visualiza√ß√£o da Rede (espessura vari√°vel)\n",
    "print('\\nCriando visualiza√ß√£o da rede...\\n')\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "# Limitar a top N pares\n",
    "TOP_N = min(50, len(df_pares_sig))\n",
    "df_viz = df_pares_sig.head(TOP_N)\n",
    "\n",
    "print(f'Visualizando top {TOP_N} pares')\n",
    "\n",
    "# Criar grafo\n",
    "G = nx.Graph()\n",
    "\n",
    "for _, row in df_viz.iterrows():\n",
    "    G.add_edge(\n",
    "        row['colab_i'],\n",
    "        row['colab_j'],\n",
    "        weight=row['jaccard'],\n",
    "        cooccur=row['cooccur']\n",
    "    )\n",
    "\n",
    "print(f'N√≥s: {G.number_of_nodes()}, Arestas: {G.number_of_edges()}')\n",
    "\n",
    "# Layout\n",
    "pos = nx.spring_layout(G, k=1, iterations=50, seed=42)\n",
    "\n",
    "# ARESTAS COM ESPESSURA VARI√ÅVEL\n",
    "edge_traces = []\n",
    "\n",
    "for edge in G.edges(data=True):\n",
    "    x0, y0 = pos[edge[0]]\n",
    "    x1, y1 = pos[edge[1]]\n",
    "\n",
    "    # Espessura proporcional ao Jaccard\n",
    "    weight = edge[2]['weight']\n",
    "    line_width = 0.5 + weight * 8  # 0.5 a 8.5\n",
    "\n",
    "    edge_trace = go.Scatter(\n",
    "        x=[x0, x1, None],\n",
    "        y=[y0, y1, None],\n",
    "        line=dict(width=line_width, color='rgba(100,100,100,0.5)'),\n",
    "        hoverinfo='none',\n",
    "        mode='lines'\n",
    "    )\n",
    "    edge_traces.append(edge_trace)\n",
    "\n",
    "# N√≥s\n",
    "node_x = []\n",
    "node_y = []\n",
    "for node in G.nodes():\n",
    "    x, y = pos[node]\n",
    "    node_x.append(x)\n",
    "    node_y.append(y)\n",
    "\n",
    "node_trace = go.Scatter(\n",
    "    x=node_x, y=node_y,\n",
    "    mode='markers',\n",
    "    hoverinfo='text',\n",
    "    marker=dict(\n",
    "        showscale=True,\n",
    "        colorscale='YlOrRd',\n",
    "        size=15,\n",
    "        colorbar=dict(\n",
    "            thickness=15,\n",
    "            title='Conex√µes',\n",
    "            xanchor='left',\n",
    "            titleside='right'\n",
    "        ),\n",
    "        line_width=2\n",
    "    )\n",
    ")\n",
    "\n",
    "# Colorir por n√∫mero de conex√µes\n",
    "node_adjacencies = []\n",
    "node_text = []\n",
    "\n",
    "for node in G.nodes():\n",
    "    adjacencies = list(G.neighbors(node))\n",
    "    node_adjacencies.append(len(adjacencies))\n",
    "\n",
    "    nome = df[df['login_colaborador'] == node]['nome_colaborador']\n",
    "    nome_str = nome.iloc[0] if len(nome) > 0 else node\n",
    "\n",
    "    node_text.append(f'{nome_str}<br>Conex√µes: {len(adjacencies)}')\n",
    "\n",
    "node_trace.marker.color = node_adjacencies\n",
    "node_trace.text = node_text\n",
    "\n",
    "# Figura\n",
    "fig = go.Figure(\n",
    "    data=edge_traces + [node_trace],\n",
    "    layout=go.Layout(\n",
    "        title=f'Rede de Co-Aus√™ncias - Jaccard Index (Top {TOP_N})',\n",
    "        titlefont_size=16,\n",
    "        showlegend=False,\n",
    "        hovermode='closest',\n",
    "        margin=dict(b=20, l=5, r=5, t=40),\n",
    "        xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "        yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "        height=700,\n",
    "        width=1000\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print('\\n‚úì Rede visualizada com espessura vari√°vel')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9B. AN√ÅLISE DE SAZONALIDADE\n",
    "\n",
    "Identificar padr√µes temporais e sazonais nas aus√™ncias.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9B.1 Heatmap: M√™s √ó Dia da Semana\n",
    "print('=== AN√ÅLISE DE SAZONALIDADE ===\\n')\n",
    "\n",
    "# Preparar dados - apenas aus√™ncias\n",
    "df_season = df.copy()\n",
    "\n",
    "# Expandir Nivel 1 se for lista\n",
    "if isinstance(df_season['Nivel 1'].iloc[0], list):\n",
    "    df_season = df_season.explode('Nivel 1')\n",
    "\n",
    "df_season = df_season[\n",
    "    df_season['Nivel 1'].isin(['Falta Justificada', 'Falta Injustificada', 'Aus√™ncia'])\n",
    "].copy()\n",
    "\n",
    "# Extrair componentes temporais\n",
    "df_season['mes'] = df_season['Data'].dt.month\n",
    "df_season['dia_semana'] = df_season['Data'].dt.dayofweek  # 0=Segunda\n",
    "df_season['mes_nome'] = df_season['Data'].dt.strftime('%b')\n",
    "df_season['dia_nome'] = df_season['Data'].dt.strftime('%a')\n",
    "\n",
    "# Criar pivot\n",
    "heatmap_data = df_season.groupby(['mes', 'dia_semana']).size().reset_index(name='count')\n",
    "heatmap_pivot = heatmap_data.pivot(index='dia_semana', columns='mes', values='count').fillna(0)\n",
    "\n",
    "# Labels\n",
    "dia_nomes = ['Seg', 'Ter', 'Qua', 'Qui', 'Sex', 'S√°b', 'Dom']\n",
    "mes_nomes = ['Jan', 'Fev', 'Mar', 'Abr', 'Mai', 'Jun', 'Jul', 'Ago', 'Set', 'Out', 'Nov', 'Dez']\n",
    "\n",
    "# Heatmap\n",
    "fig = go.Figure(data=go.Heatmap(\n",
    "    z=heatmap_pivot.values,\n",
    "    x=[mes_nomes[int(i)-1] for i in heatmap_pivot.columns],\n",
    "    y=dia_nomes,\n",
    "    colorscale='Reds',\n",
    "    colorbar=dict(title='Aus√™ncias')\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Heatmap de Aus√™ncias: M√™s √ó Dia da Semana',\n",
    "    xaxis_title='M√™s',\n",
    "    yaxis_title='Dia da Semana',\n",
    "    height=500,\n",
    "    width=1000\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print('‚úì Heatmap criado')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9B.2 Decomposi√ß√£o Temporal (Trend + Seasonal)\n",
    "print('\\nDecompondo s√©rie temporal...\\n')\n",
    "\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "# S√©rie temporal di√°ria\n",
    "ts_data = df_season.groupby('Data').size().reset_index(name='ausencias')\n",
    "ts_data = ts_data.set_index('Data').sort_index()\n",
    "ts_data = ts_data.asfreq('D', fill_value=0)\n",
    "\n",
    "print(f'Per√≠odo: {ts_data.index.min().date()} at√© {ts_data.index.max().date()}')\n",
    "print(f'Total dias: {len(ts_data)}')\n",
    "\n",
    "try:\n",
    "    # Decomposi√ß√£o (per√≠odo semanal = 7)\n",
    "    decomposition = seasonal_decompose(ts_data['ausencias'], model='additive', period=7)\n",
    "\n",
    "    # Visualizar\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Original\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=ts_data.index, y=ts_data['ausencias'],\n",
    "        mode='lines', name='Original',\n",
    "        line=dict(color='blue', width=1)\n",
    "    ))\n",
    "\n",
    "    # Trend\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=ts_data.index, y=decomposition.trend,\n",
    "        mode='lines', name='Trend',\n",
    "        line=dict(color='red', width=2)\n",
    "    ))\n",
    "\n",
    "    # Seasonal\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=ts_data.index, y=decomposition.seasonal,\n",
    "        mode='lines', name='Seasonal',\n",
    "        line=dict(color='green', width=1)\n",
    "    ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title='Decomposi√ß√£o Temporal de Aus√™ncias',\n",
    "        xaxis_title='Data',\n",
    "        yaxis_title='Aus√™ncias',\n",
    "        height=600,\n",
    "        hovermode='x unified'\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "    print('\\n‚úì Decomposi√ß√£o criada')\n",
    "    print(f'\\nüìä Insights:')\n",
    "    print(f'   M√©dia: {ts_data[\"ausencias\"].mean():.1f} aus√™ncias/dia')\n",
    "    print(f'   Tend√™ncia final: {decomposition.trend.dropna().iloc[-30:].mean():.1f}')\n",
    "    print(f'   Amplitude sazonal: {decomposition.seasonal.max() - decomposition.seasonal.min():.1f}')\n",
    "\n",
    "except Exception as e:\n",
    "    print(f'‚ö†Ô∏è  Erro na decomposi√ß√£o: {e}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. EVENT DETECTION & ANOMALY DETECTION\n",
    "\n",
    "**Objetivo**: Monitorar evolu√ß√£o temporal e detetar **mudan√ßas de padr√£o** (changepoints).\n",
    "\n",
    "**M√©todos**:\n",
    "- **U-Chart**: Control chart para taxa por unidade de tempo\n",
    "- **Rolling statistics**: M√©dia m√≥vel e desvio padr√£o\n",
    "- **Anomaly detection**: Per√≠odos estatisticamente an√≥malos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10.1 U-Chart - Taxa de aus√™ncias por semana\n",
    "print('Criando U-Chart (control chart)...')\n",
    "\n",
    "# Agrupar por semana\n",
    "df_ausencias['semana'] = df_ausencias['Data'].dt.to_period('W').astype(str)\n",
    "\n",
    "ausencias_semana = df_ausencias.groupby('semana').size().reset_index(name='num_ausencias')\n",
    "ausencias_semana['semana_dt'] = pd.to_datetime(ausencias_semana['semana'].str.split('/').str[0])\n",
    "\n",
    "# Calcular colaboradores ativos por semana (para normalizar)\n",
    "colab_semana = df.groupby(df['Data'].dt.to_period('W').astype(str))['login_colaborador'].nunique()\n",
    "ausencias_semana['colaboradores_ativos'] = ausencias_semana['semana'].map(colab_semana)\n",
    "\n",
    "# Taxa de aus√™ncias por colaborador\n",
    "ausencias_semana['taxa_ausencia_colab'] = (\n",
    "    ausencias_semana['num_ausencias'] / ausencias_semana['colaboradores_ativos']\n",
    ")\n",
    "\n",
    "# Limites de controle (U-chart)\n",
    "# UCL/LCL = mean ¬± 3*sqrt(mean/n)\n",
    "mean_taxa = ausencias_semana['taxa_ausencia_colab'].mean()\n",
    "ausencias_semana['ucl'] = mean_taxa + 3 * np.sqrt(mean_taxa / ausencias_semana['colaboradores_ativos'])\n",
    "ausencias_semana['lcl'] = mean_taxa - 3 * np.sqrt(mean_taxa / ausencias_semana['colaboradores_ativos'])\n",
    "ausencias_semana['lcl'] = ausencias_semana['lcl'].clip(lower=0)\n",
    "\n",
    "# Identificar semanas fora de controle\n",
    "ausencias_semana['out_of_control'] = (\n",
    "    (ausencias_semana['taxa_ausencia_colab'] > ausencias_semana['ucl']) |\n",
    "    (ausencias_semana['taxa_ausencia_colab'] < ausencias_semana['lcl'])\n",
    ")\n",
    "\n",
    "n_out = ausencias_semana['out_of_control'].sum()\n",
    "print(f'\\nüìä U-CHART')\n",
    "print(f'   Semanas analisadas: {len(ausencias_semana)}')\n",
    "print(f'   Taxa m√©dia: {mean_taxa:.3f} aus√™ncias/colaborador/semana')\n",
    "print(f'   Semanas FORA DE CONTROLE: {n_out} ({n_out/len(ausencias_semana)*100:.2f}%)')\n",
    "\n",
    "# Visualizar U-Chart\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=ausencias_semana['semana_dt'],\n",
    "    y=ausencias_semana['taxa_ausencia_colab'],\n",
    "    mode='lines+markers',\n",
    "    name='Taxa Observada',\n",
    "    line=dict(color='blue'),\n",
    "    marker=dict(\n",
    "        size=6,\n",
    "        color=ausencias_semana['out_of_control'].map({True: 'red', False: 'blue'})\n",
    "    )\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=ausencias_semana['semana_dt'],\n",
    "    y=[mean_taxa] * len(ausencias_semana),\n",
    "    mode='lines',\n",
    "    name='M√©dia',\n",
    "    line=dict(color='green', dash='dash')\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=ausencias_semana['semana_dt'],\n",
    "    y=ausencias_semana['ucl'],\n",
    "    mode='lines',\n",
    "    name='UCL (Upper Control Limit)',\n",
    "    line=dict(color='red', dash='dot')\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=ausencias_semana['semana_dt'],\n",
    "    y=ausencias_semana['lcl'],\n",
    "    mode='lines',\n",
    "    name='LCL (Lower Control Limit)',\n",
    "    line=dict(color='red', dash='dot'),\n",
    "    fill='tonexty',\n",
    "    fillcolor='rgba(255,0,0,0.1)'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='U-Chart: Taxa de Aus√™ncias por Semana (Control Chart)',\n",
    "    xaxis_title='Semana',\n",
    "    yaxis_title='Aus√™ncias / Colaborador',\n",
    "    height=500,\n",
    "    hovermode='x unified'\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "if n_out > 0:\n",
    "    print(f'\\nSemanas fora de controle:')\n",
    "    print(ausencias_semana[ausencias_semana['out_of_control']][\n",
    "        ['semana', 'num_ausencias', 'colaboradores_ativos', 'taxa_ausencia_colab', 'ucl']\n",
    "    ].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 11. VISUALIZA√á√ïES AVAN√áADAS\n",
    "\n",
    "### 11.1 Calendar Heatmap - Visualizar dias cr√≠ticos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11.1 Calendar Heatmap por Opera√ß√£o\n",
    "print('Criando calendar heatmap...')\n",
    "\n",
    "if 'operacao' in df_ausencias.columns:\n",
    "    # Selecionar top 3 opera√ß√µes por n√∫mero de aus√™ncias\n",
    "    top_operacoes = df_ausencias['operacao'].value_counts().head(3).index.tolist()\n",
    "    \n",
    "    for operacao in top_operacoes:\n",
    "        print(f'\\nCriando heatmap para: {operacao}')\n",
    "        \n",
    "        df_op = df_ausencias[df_ausencias['operacao'] == operacao].copy()\n",
    "        \n",
    "        # Contar aus√™ncias por dia\n",
    "        ausencias_dia = df_op.groupby('Data').size().reset_index(name='num_ausencias')\n",
    "        \n",
    "        # Adicionar features para heatmap\n",
    "        ausencias_dia['ano'] = ausencias_dia['Data'].dt.year\n",
    "        ausencias_dia['semana'] = ausencias_dia['Data'].dt.isocalendar().week\n",
    "        ausencias_dia['dia_semana'] = ausencias_dia['Data'].dt.dayofweek\n",
    "        \n",
    "        # Pivot para heatmap: semanas x dias da semana\n",
    "        pivot = ausencias_dia.pivot_table(\n",
    "            index='semana',\n",
    "            columns='dia_semana',\n",
    "            values='num_ausencias',\n",
    "            aggfunc='sum'\n",
    "        ).fillna(0)\n",
    "        \n",
    "        # Criar heatmap\n",
    "        fig = go.Figure(data=go.Heatmap(\n",
    "            z=pivot.values,\n",
    "            x=['Segunda', 'Ter√ßa', 'Quarta', 'Quinta', 'Sexta', 'S√°bado', 'Domingo'],\n",
    "            y=pivot.index,\n",
    "            colorscale='Reds',\n",
    "            hoverongaps=False,\n",
    "            hovertemplate='Semana %{y}<br>%{x}<br>Aus√™ncias: %{z}<extra></extra>'\n",
    "        ))\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title=f'Calendar Heatmap: Aus√™ncias - {operacao}',\n",
    "            xaxis_title='Dia da Semana',\n",
    "            yaxis_title='Semana do Ano',\n",
    "            height=600\n",
    "        )\n",
    "        \n",
    "        fig.show()\n",
    "else:\n",
    "    print('\\n‚ö†Ô∏è  Campo \"operacao\" n√£o encontrado')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 12. S√çNTESE EXECUTIVA E A√á√ïES RECOMENDADAS\n",
    "\n",
    "**Esta sec√ß√£o consolida os principais insights e recomenda√ß√µes para a√ß√£o.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12.1 Dashboard Executivo\n",
    "print('='*100)\n",
    "print('S√çNTESE EXECUTIVA - AN√ÅLISE DE ABSENTISMO')\n",
    "print('='*100)\n",
    "\n",
    "print(f'\\nüéØ KPIs PRINCIPAIS')\n",
    "print(f'   Taxa de Absentismo Global: {taxa_absentismo_global:.2f}%')\n",
    "print(f'   Lost Time Rate: {lost_time_rate:.2f} dias/colaborador')\n",
    "print(f'   Frequency Rate: {frequency_rate:.2f} spells/colaborador')\n",
    "print(f'   Mean Spell Duration: {mean_spell_duration:.2f} dias')\n",
    "\n",
    "print(f'\\nüö® ALERTAS CR√çTICOS')\n",
    "print(f'   Colaboradores Bradford >500 (a√ß√£o disciplinar): {(df_bradford[\"bradford_score\"] > 500).sum()}')\n",
    "print(f'   Colaboradores Bradford >900 (preocupa√ß√£o s√©ria): {(df_bradford[\"bradford_score\"] > 900).sum()}')\n",
    "\n",
    "if 'df_padroes_colab' in locals():\n",
    "    print(f'   Padr√£o Segunda/Sexta suspeito: {df_padroes_colab[\"flag_weekend_pattern\"].sum()} colaboradores')\n",
    "\n",
    "if 'df_ponte_colab' in locals():\n",
    "    print(f'   Padr√£o de Ponte suspeito: {df_ponte_colab[\"flag_bridge_pattern\"].sum()} colaboradores')\n",
    "\n",
    "print(f'   Outliers estat√≠sticos: {n_outliers} colaboradores')\n",
    "\n",
    "if 'df_surtos' in locals():\n",
    "    print(f'   Dias com surto de aus√™ncias: {len(df_surtos)} dias')\n",
    "\n",
    "if 'ausencias_semana' in locals():\n",
    "    print(f'   Semanas fora de controle (U-Chart): {n_out} semanas')\n",
    "\n",
    "print(f'\\nüìä PERFIS DE ABSENTISMO (Clusters)')\n",
    "if 'df_cluster' in locals():\n",
    "    for cluster in sorted(df_cluster['cluster_nome'].unique()):\n",
    "        n = (df_cluster['cluster_nome'] == cluster).sum()\n",
    "        pct = n / len(df_cluster) * 100\n",
    "        print(f'   {cluster}: {n} colaboradores ({pct:.1f}%)')\n",
    "\n",
    "print(f'\\nüí° A√á√ïES RECOMENDADAS')\n",
    "print(f'\\n1. IMEDIATAS (pr√≥ximas 2 semanas):')\n",
    "print(f'   - Conversa com Top 20 Bradford Factor (>= posi√ß√£o de aviso escrito)')\n",
    "print(f'   - Investigar {len(df_surtos) if \"df_surtos\" in locals() else 0} dias de surto identificados')\n",
    "print(f'   - Rever casos de padr√£o segunda/sexta e ponte')\n",
    "\n",
    "print(f'\\n2. CURTO PRAZO (pr√≥ximo m√™s):')\n",
    "print(f'   - Implementar monitoriza√ß√£o cont√≠nua (U-Chart semanal)')\n",
    "print(f'   - Definir pol√≠tica de follow-up por cluster')\n",
    "print(f'   - An√°lise detalhada das {n_out if \"ausencias_semana\" in locals() else 0} semanas fora de controle')\n",
    "\n",
    "print(f'\\n3. M√âDIO PRAZO (pr√≥ximos 3 meses):')\n",
    "print(f'   - Programa de engagement para clusters alto/muito alto')\n",
    "print(f'   - Investigar causas raiz por opera√ß√£o')\n",
    "print(f'   - Implementar sistema de early warning (Bradford + padr√µes)')\n",
    "\n",
    "print(f'\\n' + '='*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìù NOTAS FINAIS\n",
    "\n",
    "**Ficheiros Gerados**:\n",
    "- `incompatibilidades_encontradas_v2.xlsx` - Casos de dados inconsistentes removidos\n",
    "- `matriz_compatibilidade_nivel2_v2.xlsx` - Regras de compatibilidade aplicadas\n",
    "- `bradford_factor_top20.xlsx` - Colaboradores priorit√°rios para RH\n",
    "- `colaboradores_clusters_absentismo.xlsx` - Segmenta√ß√£o completa\n",
    "- `dias_surto_ausencias.xlsx` - Dias com aus√™ncias an√≥malas\n",
    "\n",
    "**Pr√≥ximos Passos**:\n",
    "1. Validar resultados com RH\n",
    "2. Criar dashboard interativo (Power BI / Tableau)\n",
    "3. Implementar monitoriza√ß√£o cont√≠nua\n",
    "4. Desenvolver ferramenta Excel para consulta (Parte 2)\n",
    "\n",
    "**Refer√™ncias**:\n",
    "- Bradford Factor: Wikipedia, Call Centre Helper\n",
    "- Spell Analysis: Fitzgerald Human Resources\n",
    "- HR Analytics: AIHR (Academy to Innovate HR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
