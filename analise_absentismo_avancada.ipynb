{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AN√ÅLISE AVAN√áADA DE ABSENTISMO\n",
    "\n",
    "**Framework Anal√≠tico Completo**: Da Descri√ß√£o √† Prescri√ß√£o\n",
    "\n",
    "---\n",
    "\n",
    "## Estrutura da An√°lise\n",
    "\n",
    "1. **Prepara√ß√£o e Limpeza de Dados**\n",
    "2. **Descri√ß√£o Fundamental dos Dados**\n",
    "3. **Conceito de Spells (Epis√≥dios de Aus√™ncia)**\n",
    "4. **M√©tricas Core (KPIs Essenciais)**\n",
    "5. **Bradford Factor Analysis**\n",
    "6. **Dete√ß√£o de Padr√µes Suspeitos**\n",
    "7. **An√°lise de Cohorts (por Data de Ingresso)**\n",
    "8. **Clustering de Perfis de Absentismo**\n",
    "9. **Network Analysis (Coincid√™ncias)**\n",
    "10. **Event Detection & Anomaly Detection**\n",
    "11. **Visualiza√ß√µes Avan√ßadas**\n",
    "12. **S√≠ntese Executiva e A√ß√µes Recomendadas**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. PREPARA√á√ÉO E LIMPEZA DE DADOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configura√ß√µes de visualiza√ß√£o\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "print('‚úì Bibliotecas carregadas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 Carregar dados\n",
    "print('Carregando dados...')\n",
    "df_raw = pd.read_csv('combined_data.csv')\n",
    "df_raw['Data'] = pd.to_datetime(df_raw['Data'])\n",
    "\n",
    "print(f'   Registos totais: {len(df_raw):,}')\n",
    "print(f'   Colaboradores √∫nicos: {df_raw[\"login_colaborador\"].nunique():,}')\n",
    "print(f'   Per√≠odo: {df_raw[\"Data\"].min().date()} at√© {df_raw[\"Data\"].max().date()}')\n",
    "\n",
    "# Carregar NOVOS c√≥digos (V2)\n",
    "print('\\nCarregando nova classifica√ß√£o (C√≥digos_V2)...')\n",
    "df_codigos = pd.read_excel('c√≥digos_V2.xlsx')\n",
    "print(f'   C√≥digos carregados: {len(df_codigos)}')\n",
    "print(f'\\nNivel 1 categories: {df_codigos[\"Nivel 1\"].nunique()}')\n",
    "print(df_codigos['Nivel 1'].value_counts())\n",
    "print(f'\\nNivel 2 categories: {df_codigos[\"Nivel 2\"].nunique()}')\n",
    "print(df_codigos['Nivel 2'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2 Merge com novos c√≥digos\n",
    "print('Aplicando nova classifica√ß√£o aos dados...')\n",
    "\n",
    "# Merge\n",
    "df_raw = df_raw.merge(\n",
    "    df_codigos,\n",
    "    left_on='segmento_processado_codigo',\n",
    "    right_on='Codigo Segmento',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Verificar c√≥digos n√£o mapeados\n",
    "codigos_sem_match = df_raw[df_raw['Nivel 1'].isna()]['segmento_processado_codigo'].unique()\n",
    "if len(codigos_sem_match) > 0:\n",
    "    print(f'\\n‚ö†Ô∏è  ATEN√á√ÉO: {len(codigos_sem_match)} c√≥digos sem correspond√™ncia:')\n",
    "    print(codigos_sem_match)\n",
    "    print(f'   Registos afetados: {df_raw[\"Nivel 1\"].isna().sum():,}')\n",
    "else:\n",
    "    print('\\n‚úì Todos os c√≥digos mapeados com sucesso')\n",
    "\n",
    "print(f'\\nTotal de registos: {len(df_raw):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Identificar e Remover Incompatibilidades\n",
    "\n",
    "**Particularidade dos dados**: Num mesmo dia, um colaborador pode ter **m√∫ltiplos registos**.\n",
    "\n",
    "**Regras de Compatibilidade** (Nivel 2):\n",
    "- ‚úÖ **Atraso** + Presen√ßa\n",
    "- ‚úÖ **Atraso** + Exame Escolar  \n",
    "- ‚úÖ **Presen√ßa** + Exame Escolar\n",
    "- ‚ùå **Tudo o resto √© INCOMPAT√çVEL**\n",
    "\n",
    "Vamos criar uma matriz de compatibilidade e identificar dias problem√°ticos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3.1 Criar matriz de compatibilidade (Nivel 2)\n",
    "print('Criando matriz de compatibilidade (Nivel 2)...')\n",
    "\n",
    "# Lista de categorias Nivel 2\n",
    "lista_nivel2 = sorted(df_codigos['Nivel 2'].unique())\n",
    "print(f'\\nCategorias Nivel 2: {len(lista_nivel2)}')\n",
    "for cat in lista_nivel2:\n",
    "    print(f'  - {cat}')\n",
    "\n",
    "# Criar matriz (default: INCOMPAT√çVEL)\n",
    "matriz_compat = pd.DataFrame('Incompat√≠vel', index=lista_nivel2, columns=lista_nivel2)\n",
    "\n",
    "# REGRA 1: Categoria consigo mesma = COMPAT√çVEL\n",
    "for cat in lista_nivel2:\n",
    "    matriz_compat.loc[cat, cat] = 'Compat√≠vel'\n",
    "\n",
    "# REGRA 2: Apenas 3 pares compat√≠veis\n",
    "pares_compativeis = [\n",
    "    ('Atraso', 'Presen√ßa'),\n",
    "    ('Atraso', 'Exame Escolar'),\n",
    "    ('Presen√ßa', 'Exame Escolar')\n",
    "]\n",
    "\n",
    "for cat1, cat2 in pares_compativeis:\n",
    "    if cat1 in matriz_compat.index and cat2 in matriz_compat.columns:\n",
    "        matriz_compat.loc[cat1, cat2] = 'Compat√≠vel'\n",
    "        matriz_compat.loc[cat2, cat1] = 'Compat√≠vel'  # Sim√©trica\n",
    "\n",
    "# Exportar matriz para valida√ß√£o\n",
    "with pd.ExcelWriter('matriz_compatibilidade_nivel2_v2.xlsx') as writer:\n",
    "    matriz_compat.to_excel(writer, sheet_name='Matriz')\n",
    "    \n",
    "print('\\n‚úì Matriz de compatibilidade criada e exportada')\n",
    "print(f'\\nPares COMPAT√çVEIS (al√©m da diagonal):' )\n",
    "for cat1, cat2 in pares_compativeis:\n",
    "    print(f'  ‚úì {cat1} + {cat2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3.2 Identificar dias com m√∫ltiplos registos\n",
    "print('Identificando dias com m√∫ltiplos registos...')\n",
    "\n",
    "registros_por_dia = df_raw.groupby(['login_colaborador', 'Data']).size()\n",
    "dias_duplicados = registros_por_dia[registros_por_dia > 1]\n",
    "\n",
    "print(f'   Dias com m√∫ltiplos registos: {len(dias_duplicados):,}')\n",
    "print(f'   ({len(dias_duplicados) / len(df_raw.groupby([\"login_colaborador\", \"Data\"])) * 100:.2f}% do total de dias-colaborador)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3.3 Testar incompatibilidades (VERS√ÉO OTIMIZADA)\n",
    "print('Testando incompatibilidades nos dias duplicados...')\n",
    "\n",
    "# PR√â-FILTRAR apenas dias duplicados\n",
    "dias_dup_list = dias_duplicados.index.tolist()\n",
    "df_dup = df_raw.set_index(['login_colaborador', 'Data']).loc[dias_dup_list].reset_index()\n",
    "\n",
    "print(f'   Registos a testar: {len(df_dup):,}')\n",
    "\n",
    "# PR√â-AGRUPAR dados (UMA vez!)\n",
    "df_dup_grouped = df_dup.groupby(['login_colaborador', 'Data']).apply(\n",
    "    lambda g: pd.Series({\n",
    "        'categorias_nivel2': list(g['Nivel 2'].dropna().unique()),\n",
    "        'codigos': list(g['segmento_processado_codigo'].unique()),\n",
    "        'nome': g['nome_colaborador'].iloc[0]\n",
    "    })\n",
    ").reset_index()\n",
    "\n",
    "print(f'   Dias √∫nicos a testar: {len(df_dup_grouped):,}')\n",
    "\n",
    "# Identificar incompatibilidades\n",
    "print('\\nTestando pares de categorias...')\n",
    "dias_incompativeis = []\n",
    "\n",
    "for idx, row in df_dup_grouped.iterrows():\n",
    "    if idx % 5000 == 0 and idx > 0:\n",
    "        print(f'   Processados {idx:,}/{len(df_dup_grouped):,} dias...')\n",
    "    \n",
    "    login = row['login_colaborador']\n",
    "    data = row['Data']\n",
    "    categorias = row['categorias_nivel2']\n",
    "    codigos = row['codigos']\n",
    "    nome = row['nome']\n",
    "    \n",
    "    # Testar todos os pares\n",
    "    incompativel_encontrado = False\n",
    "    pares_incompativeis = []\n",
    "    \n",
    "    for i, cat1 in enumerate(categorias):\n",
    "        for cat2 in categorias[i+1:]:\n",
    "            if cat1 in matriz_compat.index and cat2 in matriz_compat.columns:\n",
    "                if matriz_compat.loc[cat1, cat2] == 'Incompat√≠vel':\n",
    "                    incompativel_encontrado = True\n",
    "                    pares_incompativeis.append(f'{cat1} + {cat2}')\n",
    "    \n",
    "    if incompativel_encontrado:\n",
    "        dias_incompativeis.append({\n",
    "            'login_colaborador': login,\n",
    "            'Data': data,\n",
    "            'nome_colaborador': nome,\n",
    "            'categorias': ', '.join(categorias),\n",
    "            'codigos': ', '.join(codigos),\n",
    "            'pares_incompativeis': ' | '.join(pares_incompativeis)\n",
    "        })\n",
    "\n",
    "df_incompativeis = pd.DataFrame(dias_incompativeis)\n",
    "\n",
    "print(f'\\n‚úì Teste conclu√≠do')\n",
    "print(f'\\nüî¥ INCOMPATIBILIDADES ENCONTRADAS: {len(df_incompativeis)}')\n",
    "\n",
    "if len(df_incompativeis) > 0:\n",
    "    print(f'\\nDistribui√ß√£o por par incompat√≠vel:')\n",
    "    for par in df_incompativeis['pares_incompativeis'].str.split(' | ').explode().value_counts().head(10).items():\n",
    "        print(f'   {par[0]}: {par[1]} casos')\n",
    "    \n",
    "    # Exportar para Excel\n",
    "    df_incompativeis.to_excel('incompatibilidades_encontradas_v2.xlsx', index=False)\n",
    "    print('\\n‚úì Detalhes exportados para: incompatibilidades_encontradas_v2.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3.4 Remover dias incompat√≠veis (VERS√ÉO VETORIZADA)\n",
    "print('Removendo dias incompat√≠veis...')\n",
    "\n",
    "n_antes = len(df_raw)\n",
    "\n",
    "if len(df_incompativeis) > 0:\n",
    "    # Criar DataFrame dos dias a remover\n",
    "    dias_remover = df_incompativeis[['login_colaborador', 'Data']].copy()\n",
    "    \n",
    "    # Merge com indicador\n",
    "    df_temp = df_raw.merge(dias_remover, on=['login_colaborador', 'Data'], how='left', indicator=True)\n",
    "    \n",
    "    # Manter apenas linhas N√ÉO marcadas\n",
    "    df_limpo = df_temp[df_temp['_merge'] == 'left_only'].drop('_merge', axis=1)\n",
    "else:\n",
    "    df_limpo = df_raw.copy()\n",
    "    print('   Nenhuma incompatibilidade encontrada')\n",
    "\n",
    "n_depois = len(df_limpo)\n",
    "print(f'\\n‚úì Registos removidos: {n_antes - n_depois:,}')\n",
    "print(f'   ({(n_antes - n_depois) / n_antes * 100:.3f}% do total)')\n",
    "print(f'\\nDataset limpo: {n_depois:,} registos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3.5 Agrega√ß√£o final: 1 linha por dia-colaborador\n",
    "print('Agregando dados: 1 linha por dia-colaborador...')\n",
    "\n",
    "# Definir regras de agrega√ß√£o\n",
    "agg_rules = {\n",
    "    'nome_colaborador': 'first',\n",
    "    'categoria_profissional': 'first',\n",
    "    'segmento_processado_codigo': lambda x: ', '.join(x.unique()),  # Concatenar se m√∫ltiplos\n",
    "    'Nivel 1': lambda x: ', '.join(x.dropna().unique()),\n",
    "    'Nivel 2': lambda x: ', '.join(x.dropna().unique()),\n",
    "}\n",
    "\n",
    "# Adicionar opera√ß√£o se existir\n",
    "if 'operacao' in df_limpo.columns:\n",
    "    agg_rules['operacao'] = 'first'\n",
    "\n",
    "df = df_limpo.groupby(['login_colaborador', 'Data']).agg(agg_rules).reset_index()\n",
    "\n",
    "print(f'\\n‚úì Dataset agregado: {len(df):,} dias-colaborador')\n",
    "print(f'   Colaboradores √∫nicos: {df[\"login_colaborador\"].nunique():,}')\n",
    "print(f'   Per√≠odo: {df[\"Data\"].min().date()} at√© {df[\"Data\"].max().date()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. DESCRI√á√ÉO FUNDAMENTAL DOS DADOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 Estat√≠sticas descritivas\n",
    "print('=== DESCRI√á√ÉO DO DATASET ===')\n",
    "print(f'\\nüìä DIMENS√ÉO')\n",
    "print(f'   Total de registos (dias-colaborador): {len(df):,}')\n",
    "print(f'   Colaboradores √∫nicos: {df[\"login_colaborador\"].nunique():,}')\n",
    "print(f'   Per√≠odo: {df[\"Data\"].min().date()} at√© {df[\"Data\"].max().date()}')\n",
    "print(f'   Dias calend√°rio: {(df[\"Data\"].max() - df[\"Data\"].min()).days + 1}')\n",
    "\n",
    "print(f'\\nüìã DISTRIBUI√á√ÉO POR NIVEL 1')\n",
    "dist_nivel1 = df['Nivel 1'].value_counts()\n",
    "for cat, count in dist_nivel1.items():\n",
    "    pct = count / len(df) * 100\n",
    "    print(f'   {cat:25s}: {count:7,} ({pct:5.2f}%)')\n",
    "\n",
    "print(f'\\nüìã DISTRIBUI√á√ÉO POR NIVEL 2')\n",
    "dist_nivel2 = df['Nivel 2'].value_counts()\n",
    "for cat, count in dist_nivel2.head(15).items():\n",
    "    pct = count / len(df) * 100\n",
    "    print(f'   {cat:30s}: {count:7,} ({pct:5.2f}%)')\n",
    "\n",
    "if 'operacao' in df.columns:\n",
    "    print(f'\\nüè¢ OPERA√á√ïES')\n",
    "    print(f'   Opera√ß√µes √∫nicas: {df[\"operacao\"].nunique()}')\n",
    "    print(f'   Top 10 opera√ß√µes:')\n",
    "    for op, count in df['operacao'].value_counts().head(10).items():\n",
    "        pct = count / len(df) * 100\n",
    "        print(f'      {op:40s}: {count:6,} ({pct:5.2f}%)')\n",
    "\n",
    "print(f'\\nüë• CATEGORIAS PROFISSIONAIS')\n",
    "print(f'   Categorias √∫nicas: {df[\"categoria_profissional\"].nunique()}')\n",
    "for cat, count in df['categoria_profissional'].value_counts().head(10).items():\n",
    "    pct = count / len(df) * 100\n",
    "    print(f'   {cat:30s}: {count:6,} ({pct:5.2f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 Visualiza√ß√£o: Distribui√ß√£o temporal\n",
    "print('Criando visualiza√ß√£o de distribui√ß√£o temporal...')\n",
    "\n",
    "# Agrupar por m√™s\n",
    "df['Ano_Mes'] = df['Data'].dt.to_period('M').astype(str)\n",
    "df_mensal = df.groupby('Ano_Mes').size().reset_index(name='Registos')\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=df_mensal['Ano_Mes'],\n",
    "    y=df_mensal['Registos'],\n",
    "    mode='lines+markers',\n",
    "    name='Registos',\n",
    "    line=dict(color='#1f77b4', width=2),\n",
    "    marker=dict(size=6)\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Evolu√ß√£o Temporal dos Registos',\n",
    "    xaxis_title='M√™s',\n",
    "    yaxis_title='N√∫mero de Registos',\n",
    "    height=400,\n",
    "    hovermode='x unified'\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "print('‚úì Visualiza√ß√£o criada')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. CONCEITO DE SPELLS (EPIS√ìDIOS DE AUS√äNCIA)\n",
    "\n",
    "**Spell** = Epis√≥dio cont√≠nuo de aus√™ncia (um ou mais dias consecutivos).\n",
    "\n",
    "**Import√¢ncia**:\n",
    "- 1 spell de 5 dias vs 5 spells de 1 dia = **impacto operacional MUITO diferente**\n",
    "- Permite calcular m√©tricas sofisticadas:\n",
    "  - **Frequency Rate**: Taxa de spells por colaborador\n",
    "  - **Mean Spell Duration**: Dura√ß√£o m√©dia de cada epis√≥dio\n",
    "  - **Short-term spells**: ‚â§ 3 dias (poss√≠vel \"abuso\")\n",
    "  - **Long-term spells**: > 14 dias (doen√ßa grave, tratamento m√©dico)\n",
    "\n",
    "**Algoritmo**:\n",
    "1. Ordenar registos por colaborador e data\n",
    "2. Identificar quebras de continuidade (gap > 1 dia)\n",
    "3. Agrupar dias consecutivos no mesmo spell\n",
    "4. Calcular dura√ß√£o, tipo predominante, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Criar spells de AUS√äNCIA (excluir Presen√ßas e Atrasos)\n",
    "print('Criando spells de aus√™ncia...')\n",
    "\n",
    "# Filtrar apenas AUS√äNCIAS (excluir Trabalho Pago, Atraso)\n",
    "df_ausencias = df[df['Nivel 1'].isin(['Falta Justificada', 'Falta Injustificada', 'Aus√™ncia'])].copy()\n",
    "\n",
    "print(f'   Registos de aus√™ncia: {len(df_ausencias):,}')\n",
    "print(f'   Colaboradores com aus√™ncias: {df_ausencias[\"login_colaborador\"].nunique():,}')\n",
    "\n",
    "# Ordenar por colaborador e data\n",
    "df_ausencias = df_ausencias.sort_values(['login_colaborador', 'Data']).reset_index(drop=True)\n",
    "\n",
    "# Calcular diferen√ßa de dias entre registos consecutivos\n",
    "df_ausencias['dias_desde_anterior'] = df_ausencias.groupby('login_colaborador')['Data'].diff().dt.days\n",
    "\n",
    "# Novo spell quando:\n",
    "# 1. Primeiro registo do colaborador (dias_desde_anterior = NaN)\n",
    "# 2. Gap > 1 dia (n√£o consecutivo)\n",
    "df_ausencias['novo_spell'] = (\n",
    "    (df_ausencias['dias_desde_anterior'].isna()) |  # Primeiro registo\n",
    "    (df_ausencias['dias_desde_anterior'] > 1)        # Gap de dias\n",
    ")\n",
    "\n",
    "# Atribuir ID √∫nico a cada spell\n",
    "df_ausencias['spell_id'] = df_ausencias['novo_spell'].cumsum()\n",
    "\n",
    "print(f'\\n‚úì Spells identificados: {df_ausencias[\"spell_id\"].nunique():,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 Agregar informa√ß√£o por spell\n",
    "print('Agregando informa√ß√£o por spell...')\n",
    "\n",
    "df_spells = df_ausencias.groupby('spell_id').agg({\n",
    "    'login_colaborador': 'first',\n",
    "    'nome_colaborador': 'first',\n",
    "    'categoria_profissional': 'first',\n",
    "    'Data': ['min', 'max', 'count'],  # In√≠cio, fim, dura√ß√£o\n",
    "    'Nivel 1': lambda x: x.mode()[0] if len(x.mode()) > 0 else x.iloc[0],  # Tipo predominante\n",
    "    'Nivel 2': lambda x: x.mode()[0] if len(x.mode()) > 0 else x.iloc[0],\n",
    "}).reset_index()\n",
    "\n",
    "# Renomear colunas\n",
    "df_spells.columns = [\n",
    "    'spell_id', 'login_colaborador', 'nome_colaborador', 'categoria_profissional',\n",
    "    'data_inicio', 'data_fim', 'duracao_dias',\n",
    "    'nivel1_predominante', 'nivel2_predominante'\n",
    "]\n",
    "\n",
    "# Adicionar opera√ß√£o se existir\n",
    "if 'operacao' in df_ausencias.columns:\n",
    "    df_spells = df_spells.merge(\n",
    "        df_ausencias.groupby('spell_id')['operacao'].first().reset_index(),\n",
    "        on='spell_id'\n",
    "    )\n",
    "\n",
    "# Adicionar features\n",
    "df_spells['dia_semana_inicio'] = df_spells['data_inicio'].dt.day_name()\n",
    "df_spells['dia_semana_fim'] = df_spells['data_fim'].dt.day_name()\n",
    "df_spells['mes'] = df_spells['data_inicio'].dt.month\n",
    "df_spells['ano'] = df_spells['data_inicio'].dt.year\n",
    "\n",
    "# Categorizar spells\n",
    "df_spells['categoria_spell'] = pd.cut(\n",
    "    df_spells['duracao_dias'],\n",
    "    bins=[0, 1, 3, 7, 14, float('inf')],\n",
    "    labels=['1 dia', '2-3 dias', '4-7 dias', '8-14 dias', '>14 dias']\n",
    ")\n",
    "\n",
    "df_spells['short_term'] = df_spells['duracao_dias'] <= 3\n",
    "df_spells['long_term'] = df_spells['duracao_dias'] > 14\n",
    "\n",
    "print(f'\\n‚úì Dataset de spells criado: {len(df_spells):,} spells')\n",
    "print(f'\\nDistribui√ß√£o por dura√ß√£o:')\n",
    "print(df_spells['categoria_spell'].value_counts().sort_index())\n",
    "\n",
    "print(f'\\nEstat√≠sticas de dura√ß√£o:')\n",
    "print(f'   M√©dia: {df_spells[\"duracao_dias\"].mean():.2f} dias')\n",
    "print(f'   Mediana: {df_spells[\"duracao_dias\"].median():.0f} dias')\n",
    "print(f'   P75: {df_spells[\"duracao_dias\"].quantile(0.75):.0f} dias')\n",
    "print(f'   P95: {df_spells[\"duracao_dias\"].quantile(0.95):.0f} dias')\n",
    "print(f'   M√°ximo: {df_spells[\"duracao_dias\"].max()} dias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3 An√°lise de spells por colaborador\n",
    "print('Analisando spells por colaborador...')\n",
    "\n",
    "df_colab_spells = df_spells.groupby('login_colaborador').agg({\n",
    "    'spell_id': 'count',  # Frequency rate (n√∫mero de spells)\n",
    "    'duracao_dias': ['sum', 'mean', 'median', 'std'],\n",
    "    'short_term': 'sum',  # N√∫mero de spells curtos\n",
    "    'long_term': 'sum',   # N√∫mero de spells longos\n",
    "}).reset_index()\n",
    "\n",
    "df_colab_spells.columns = [\n",
    "    'login_colaborador', 'num_spells', \n",
    "    'total_dias_ausentes', 'mean_spell_duration', 'median_spell_duration', 'std_spell_duration',\n",
    "    'num_short_term_spells', 'num_long_term_spells'\n",
    "]\n",
    "\n",
    "# Adicionar nome\n",
    "df_colab_spells = df_colab_spells.merge(\n",
    "    df[['login_colaborador', 'nome_colaborador']].drop_duplicates(),\n",
    "    on='login_colaborador'\n",
    ")\n",
    "\n",
    "print(f'\\n‚úì An√°lise por colaborador criada: {len(df_colab_spells):,} colaboradores')\n",
    "print(f'\\nTop 10 colaboradores por n√∫mero de spells:')\n",
    "print(df_colab_spells.nlargest(10, 'num_spells')[[\n",
    "    'nome_colaborador', 'num_spells', 'total_dias_ausentes', 'mean_spell_duration'\n",
    "]].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. M√âTRICAS CORE (KPIs ESSENCIAIS)\n",
    "\n",
    "Baseadas em frameworks de HR Analytics (AIHR, Fitzgerald HR)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 Calcular m√©tricas fundamentais\n",
    "print('=== M√âTRICAS CORE ===')\n",
    "\n",
    "# Per√≠odo de an√°lise\n",
    "data_inicio = df['Data'].min()\n",
    "data_fim = df['Data'].max()\n",
    "dias_calendario = (data_fim - data_inicio).days + 1\n",
    "num_colaboradores = df['login_colaborador'].nunique()\n",
    "\n",
    "# Contar registos por tipo\n",
    "num_presencas = df[df['Nivel 1'] == 'Trabalho Pago'].shape[0]\n",
    "num_atrasos = df[df['Nivel 1'] == 'Atraso'].shape[0]\n",
    "num_faltas = df[df['Nivel 1'].isin(['Falta Justificada', 'Falta Injustificada'])].shape[0]\n",
    "num_ausencias_medicas = df[df['Nivel 2'] == 'Aus√™ncia M√©dica'].shape[0]\n",
    "\n",
    "# KPI 1: Taxa de Absentismo Global\n",
    "# % Absentismo = Total de Faltas / (Presen√ßas + Total de Faltas)\n",
    "taxa_absentismo_global = (num_faltas / (num_presencas + num_faltas)) * 100\n",
    "\n",
    "# KPI 2: Lost Time Rate (dias perdidos por FTE)\n",
    "total_dias_perdidos = df_spells['duracao_dias'].sum()\n",
    "lost_time_rate = total_dias_perdidos / num_colaboradores\n",
    "\n",
    "# KPI 3: Frequency Rate (spells por colaborador)\n",
    "frequency_rate = len(df_spells) / num_colaboradores\n",
    "\n",
    "# KPI 4: Mean Spell Duration\n",
    "mean_spell_duration = df_spells['duracao_dias'].mean()\n",
    "\n",
    "# KPI 5: Taxa de Atrasos\n",
    "# % Atrasos = Atrasos / Total Presen√ßas\n",
    "taxa_atrasos = (num_atrasos / num_presencas) * 100 if num_presencas > 0 else 0\n",
    "\n",
    "# KPI 6: Taxa de Zero Aus√™ncias\n",
    "colaboradores_sem_ausencias = num_colaboradores - df_spells['login_colaborador'].nunique()\n",
    "taxa_zero_ausencias = (colaboradores_sem_ausencias / num_colaboradores) * 100\n",
    "\n",
    "# Exibir resultados\n",
    "print(f'\\nüìä PER√çODO DE AN√ÅLISE')\n",
    "print(f'   {data_inicio.date()} at√© {data_fim.date()} ({dias_calendario} dias)')\n",
    "print(f'   Colaboradores √∫nicos: {num_colaboradores:,}')\n",
    "\n",
    "print(f'\\nüéØ KPIs PRINCIPAIS')\n",
    "print(f'   Taxa de Absentismo Global: {taxa_absentismo_global:.2f}%')\n",
    "print(f'   Lost Time Rate: {lost_time_rate:.2f} dias/colaborador')\n",
    "print(f'   Frequency Rate: {frequency_rate:.2f} spells/colaborador')\n",
    "print(f'   Mean Spell Duration: {mean_spell_duration:.2f} dias')\n",
    "print(f'   Taxa de Atrasos: {taxa_atrasos:.2f}%')\n",
    "print(f'   Taxa de Zero Aus√™ncias: {taxa_zero_ausencias:.2f}%')\n",
    "\n",
    "print(f'\\nüìà BREAKDOWN')\n",
    "print(f'   Presen√ßas: {num_presencas:,}')\n",
    "print(f'   Atrasos: {num_atrasos:,}')\n",
    "print(f'   Faltas (Total): {num_faltas:,}')\n",
    "print(f'      - Justificadas: {df[df[\"Nivel 1\"] == \"Falta Justificada\"].shape[0]:,}')\n",
    "print(f'      - Injustificadas: {df[df[\"Nivel 1\"] == \"Falta Injustificada\"].shape[0]:,}')\n",
    "print(f'   Aus√™ncias M√©dicas: {num_ausencias_medicas:,}')\n",
    "print(f'\\n   Total de Spells: {len(df_spells):,}')\n",
    "print(f'      - Short-term (‚â§3 dias): {df_spells[\"short_term\"].sum():,}')\n",
    "print(f'      - Long-term (>14 dias): {df_spells[\"long_term\"].sum():,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. BRADFORD FACTOR ANALYSIS\n",
    "\n",
    "**Bradford Factor** = S¬≤ √ó D\n",
    "- S = N√∫mero de spells (epis√≥dios)\n",
    "- D = Total de dias ausentes\n",
    "\n",
    "**Princ√≠pio**: Aus√™ncias curtas e frequentes s√£o **mais disruptivas** que aus√™ncias longas ocasionais.\n",
    "\n",
    "**Exemplo**:\n",
    "- Colaborador A: 1 spell de 10 dias ‚Üí B = 1¬≤ √ó 10 = **10**\n",
    "- Colaborador B: 10 spells de 1 dia ‚Üí B = 10¬≤ √ó 10 = **1000** (100x pior!)\n",
    "\n",
    "**Thresholds t√≠picos** (Call Centre Helper):\n",
    "- < 45: Aceit√°vel\n",
    "- 45-100: Conversa informal  \n",
    "- 100-200: Revis√£o formal\n",
    "- 200-500: Aviso escrito\n",
    "- 500-900: A√ß√£o disciplinar\n",
    "- \\> 900: Preocupa√ß√£o s√©ria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1 Calcular Bradford Factor por colaborador\n",
    "print('Calculando Bradford Factor...')\n",
    "\n",
    "df_bradford = df_colab_spells.copy()\n",
    "df_bradford['bradford_score'] = (df_bradford['num_spells'] ** 2) * df_bradford['total_dias_ausentes']\n",
    "\n",
    "# Categorizar por risk level\n",
    "def categorizar_bradford(score):\n",
    "    if score < 45:\n",
    "        return '1. Aceit√°vel (<45)'\n",
    "    elif score < 100:\n",
    "        return '2. Conversa Informal (45-100)'\n",
    "    elif score < 200:\n",
    "        return '3. Revis√£o Formal (100-200)'\n",
    "    elif score < 500:\n",
    "        return '4. Aviso Escrito (200-500)'\n",
    "    elif score < 900:\n",
    "        return '5. A√ß√£o Disciplinar (500-900)'\n",
    "    else:\n",
    "        return '6. Preocupa√ß√£o S√©ria (>900)'\n",
    "\n",
    "df_bradford['risk_level'] = df_bradford['bradford_score'].apply(categorizar_bradford)\n",
    "\n",
    "print(f'\\n‚úì Bradford Factor calculado para {len(df_bradford):,} colaboradores')\n",
    "print(f'\\nDistribui√ß√£o por Risk Level:')\n",
    "dist_risk = df_bradford['risk_level'].value_counts().sort_index()\n",
    "for level, count in dist_risk.items():\n",
    "    pct = count / len(df_bradford) * 100\n",
    "    print(f'   {level:40s}: {count:4,} ({pct:5.2f}%)')\n",
    "\n",
    "print(f'\\nEstat√≠sticas do Bradford Score:')\n",
    "print(f'   M√©dia: {df_bradford[\"bradford_score\"].mean():.2f}')\n",
    "print(f'   Mediana: {df_bradford[\"bradford_score\"].median():.2f}')\n",
    "print(f'   P75: {df_bradford[\"bradford_score\"].quantile(0.75):.2f}')\n",
    "print(f'   P90: {df_bradford[\"bradford_score\"].quantile(0.90):.2f}')\n",
    "print(f'   P95: {df_bradford[\"bradford_score\"].quantile(0.95):.2f}')\n",
    "print(f'   M√°ximo: {df_bradford[\"bradford_score\"].max():.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2 Top colaboradores para aten√ß√£o de RH\n",
    "print('\\nüî¥ TOP 20 COLABORADORES PARA ATEN√á√ÉO DE RH (Bradford Factor)')\n",
    "print('='*100)\n",
    "\n",
    "top20_bradford = df_bradford.nlargest(20, 'bradford_score')[\n",
    "    ['nome_colaborador', 'num_spells', 'total_dias_ausentes', \n",
    "     'mean_spell_duration', 'bradford_score', 'risk_level']\n",
    "].copy()\n",
    "\n",
    "top20_bradford.columns = ['Nome', 'N¬∫ Spells', 'Total Dias', 'Dura√ß√£o M√©dia', 'Bradford', 'Risk Level']\n",
    "\n",
    "print(top20_bradford.to_string(index=False))\n",
    "\n",
    "# Exportar para Excel\n",
    "top20_bradford.to_excel('bradford_factor_top20.xlsx', index=False)\n",
    "print('\\n‚úì Top 20 exportado para: bradford_factor_top20.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.3 Visualiza√ß√£o: Distribui√ß√£o de Bradford Factor\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=['Distribui√ß√£o por Risk Level', 'Scatter: Spells vs Dias (Bradford)']\n",
    ")\n",
    "\n",
    "# Gr√°fico 1: Barras de risk level\n",
    "dist_risk_sorted = dist_risk.sort_index()\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=[x.split('. ')[1].split(' (')[0] for x in dist_risk_sorted.index],\n",
    "        y=dist_risk_sorted.values,\n",
    "        marker_color=['green', 'yellow', 'orange', 'red', 'darkred', 'black'][:len(dist_risk_sorted)],\n",
    "        text=dist_risk_sorted.values,\n",
    "        textposition='auto'\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Gr√°fico 2: Scatter plot\n",
    "# Colorir por risk level\n",
    "color_map = {\n",
    "    '1. Aceit√°vel (<45)': 'green',\n",
    "    '2. Conversa Informal (45-100)': 'yellow',\n",
    "    '3. Revis√£o Formal (100-200)': 'orange',\n",
    "    '4. Aviso Escrito (200-500)': 'red',\n",
    "    '5. A√ß√£o Disciplinar (500-900)': 'darkred',\n",
    "    '6. Preocupa√ß√£o S√©ria (>900)': 'black'\n",
    "}\n",
    "\n",
    "for risk_level in df_bradford['risk_level'].unique():\n",
    "    df_temp = df_bradford[df_bradford['risk_level'] == risk_level]\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df_temp['num_spells'],\n",
    "            y=df_temp['total_dias_ausentes'],\n",
    "            mode='markers',\n",
    "            name=risk_level.split('. ')[1].split(' (')[0],\n",
    "            marker=dict(\n",
    "                size=8,\n",
    "                color=color_map.get(risk_level, 'gray'),\n",
    "                opacity=0.7\n",
    "            ),\n",
    "            text=df_temp['nome_colaborador'],\n",
    "            hovertemplate='<b>%{text}</b><br>Spells: %{x}<br>Dias: %{y}<extra></extra>'\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "\n",
    "fig.update_xaxes(title_text='Risk Level', row=1, col=1)\n",
    "fig.update_yaxes(title_text='N√∫mero de Colaboradores', row=1, col=1)\n",
    "fig.update_xaxes(title_text='N√∫mero de Spells', row=1, col=2)\n",
    "fig.update_yaxes(title_text='Total Dias Ausentes', row=1, col=2)\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Bradford Factor Analysis',\n",
    "    height=500,\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 5.4 Visualiza√ß√£o EXTRA: Bradford Heatmap por Opera√ß√£o e Categoria\nprint('\\nCriando heatmap Bradford por Opera√ß√£o √ó Categoria...')\n\nif 'operacao' in df.columns:\n    # Merge Bradford scores com opera√ß√£o\n    df_bradford_op = df_bradford.merge(\n        df[['login_colaborador', 'operacao']].drop_duplicates(),\n        on='login_colaborador',\n        how='left'\n    )\n\n    # Merge com categoria profissional\n    df_bradford_op = df_bradford_op.merge(\n        df[['login_colaborador', 'categoria_profissional']].drop_duplicates(),\n        on='login_colaborador',\n        how='left'\n    )\n\n    # Top 10 opera√ß√µes\n    top_ops = df['operacao'].value_counts().head(10).index\n    df_bradford_op_filt = df_bradford_op[df_bradford_op['operacao'].isin(top_ops)]\n\n    # Top 5 categorias\n    top_cats = df['categoria_profissional'].value_counts().head(5).index\n    df_bradford_op_filt = df_bradford_op_filt[df_bradford_op_filt['categoria_profissional'].isin(top_cats)]\n\n    # Pivot: opera√ß√£o √ó categoria\n    pivot_bradford = df_bradford_op_filt.pivot_table(\n        index='operacao',\n        columns='categoria_profissional',\n        values='bradford_score',\n        aggfunc='mean'\n    ).fillna(0)\n\n    # Heatmap\n    fig = go.Figure(data=go.Heatmap(\n        z=pivot_bradford.values,\n        x=pivot_bradford.columns,\n        y=pivot_bradford.index,\n        colorscale='RdYlGn_r',  # Vermelho = alto, Verde = baixo\n        text=pivot_bradford.values.round(0),\n        texttemplate='%{text}',\n        textfont={\"size\": 10},\n        hovertemplate='Opera√ß√£o: %{y}<br>Categoria: %{x}<br>Bradford M√©dio: %{z:.0f}<extra></extra>'\n    ))\n\n    fig.update_layout(\n        title='Bradford Factor M√©dio: Opera√ß√£o √ó Categoria Profissional',\n        xaxis_title='Categoria Profissional',\n        yaxis_title='Opera√ß√£o',\n        height=600,\n        width=1000\n    )\n\n    fig.show()\n    print('‚úì Heatmap criado')\nelse:\n    print('‚ö†Ô∏è Campo \"operacao\" n√£o encontrado')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 5.5 Visualiza√ß√£o EXTRA: Funil de A√ß√£o (Bradford Thresholds)\nprint('\\nCriando funil de a√ß√£o...')\n\n# Contar colaboradores por threshold\nthresholds = [\n    (0, 45, 'Aceit√°vel'),\n    (45, 100, 'Conversa Informal'),\n    (100, 200, 'Revis√£o Formal'),\n    (200, 500, 'Aviso Escrito'),\n    (500, 900, 'A√ß√£o Disciplinar'),\n    (900, float('inf'), 'Preocupa√ß√£o S√©ria')\n]\n\nfunnel_data = []\nfor min_val, max_val, label in thresholds:\n    count = ((df_bradford['bradford_score'] >= min_val) &\n             (df_bradford['bradford_score'] < max_val)).sum()\n    funnel_data.append({'N√≠vel': label, 'Colaboradores': count})\n\ndf_funnel = pd.DataFrame(funnel_data)\n\n# Criar funil (inverted para mostrar prioridade)\nfig = go.Figure()\n\ncolors = ['green', 'yellow', 'orange', 'red', 'darkred', 'black']\n\nfig.add_trace(go.Funnel(\n    y=df_funnel['N√≠vel'],\n    x=df_funnel['Colaboradores'],\n    textposition='inside',\n    textinfo='value+percent initial',\n    marker=dict(color=colors),\n    connector={\"line\": {\"color\": \"gray\", \"dash\": \"dot\", \"width\": 2}}\n))\n\nfig.update_layout(\n    title='Funil de A√ß√£o: Colaboradores por N√≠vel de Risco (Bradford Factor)',\n    height=500,\n    width=800\n)\n\nfig.show()\n\nprint('\\nüìä FUNIL DE A√á√ÉO')\nfor _, row in df_funnel.iterrows():\n    print(f'   {row[\"N√≠vel\"]:30s}: {row[\"Colaboradores\"]:4,} colaboradores')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 5.6 Visualiza√ß√£o EXTRA: Timeline de Spells (Frequ√™ncia vs Dura√ß√£o)\nprint('\\nCriando timeline de spells...')\n\n# Agrupar spells por m√™s\ndf_spells['mes_inicio'] = df_spells['data_inicio'].dt.to_period('M').astype(str)\n\nspells_mensal = df_spells.groupby('mes_inicio').agg({\n    'spell_id': 'count',\n    'duracao_dias': 'mean'\n}).reset_index()\n\nspells_mensal.columns = ['Mes', 'Num_Spells', 'Duracao_Media']\n\n# Dual-axis plot\nfig = make_subplots(specs=[[{\"secondary_y\": True}]])\n\nfig.add_trace(\n    go.Bar(\n        x=spells_mensal['Mes'],\n        y=spells_mensal['Num_Spells'],\n        name='N√∫mero de Spells',\n        marker_color='lightblue'\n    ),\n    secondary_y=False\n)\n\nfig.add_trace(\n    go.Scatter(\n        x=spells_mensal['Mes'],\n        y=spells_mensal['Duracao_Media'],\n        name='Dura√ß√£o M√©dia (dias)',\n        mode='lines+markers',\n        line=dict(color='red', width=3),\n        marker=dict(size=8)\n    ),\n    secondary_y=True\n)\n\nfig.update_xaxes(title_text='M√™s')\nfig.update_yaxes(title_text='N√∫mero de Spells', secondary_y=False)\nfig.update_yaxes(title_text='Dura√ß√£o M√©dia (dias)', secondary_y=True)\n\nfig.update_layout(\n    title='Evolu√ß√£o Temporal: Frequ√™ncia vs Dura√ß√£o de Spells',\n    height=500,\n    hovermode='x unified'\n)\n\nfig.show()\nprint('‚úì Timeline criada')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**CONTINUA NA PR√ìXIMA C√âLULA**:\n",
    "- Dete√ß√£o de Padr√µes Suspeitos\n",
    "- An√°lise de Cohorts\n",
    "- Clustering\n",
    "- Network Analysis\n",
    "- Event Detection\n",
    "- Visualiza√ß√µes Avan√ßadas"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## 6. DETE√á√ÉO DE PADR√ïES SUSPEITOS üîç\n\n**Red Flags** para poss√≠vel abuso ou comportamento an√≥malo:\n- **Weekend Pattern**: Faltas consistentes √†s sextas/segundas\n- **Bridge Pattern**: Aus√™ncias adjacentes a feriados  \n- **Temporal Consistency**: Sempre no mesmo dia/semana/m√™s\n- **Fragmenta√ß√£o**: Bradford score alto com poucos dias totais",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.1 Padr√£o Segunda/Sexta - Spells que come√ßam/terminam em fim de semana\nprint('Analisando padr√µes de segunda/sexta...')\n\n# Propor√ß√£o de spells que come√ßam √† segunda\nspells_inicio_segunda = df_spells[df_spells['dia_semana_inicio'] == 'Monday'].shape[0]\nprop_inicio_segunda = spells_inicio_segunda / len(df_spells) * 100\n\n# Propor√ß√£o de spells que terminam √† sexta\nspells_fim_sexta = df_spells[df_spells['dia_semana_fim'] == 'Friday'].shape[0]\nprop_fim_sexta = spells_fim_sexta / len(df_spells) * 100\n\n# Spells que combinam ambos (ponte de fim de semana)\nspells_ponte_fds = df_spells[\n    (df_spells['dia_semana_inicio'] == 'Monday') & \n    (df_spells['dia_semana_fim'] == 'Friday')\n].shape[0]\n\nprint(f'\\nüìä PADR√ÉO SEGUNDA/SEXTA')\nprint(f'   Spells que come√ßam √† segunda: {spells_inicio_segunda:,} ({prop_inicio_segunda:.2f}%)')\nprint(f'   Spells que terminam √† sexta: {spells_fim_sexta:,} ({prop_fim_sexta:.2f}%)')\nprint(f'   Spells \"ponte de semana\" (2¬™ ‚Üí 6¬™): {spells_ponte_fds:,}')\n\n# An√°lise por colaborador\ndf_padroes_colab = df_spells.groupby('login_colaborador').agg({\n    'spell_id': 'count',\n    'dia_semana_inicio': lambda x: (x == 'Monday').sum(),\n    'dia_semana_fim': lambda x: (x == 'Friday').sum()\n}).reset_index()\n\ndf_padroes_colab.columns = ['login_colaborador', 'total_spells', 'inicio_segunda', 'fim_sexta']\n\n# Calcular propor√ß√µes\ndf_padroes_colab['prop_inicio_segunda'] = df_padroes_colab['inicio_segunda'] / df_padroes_colab['total_spells']\ndf_padroes_colab['prop_fim_sexta'] = df_padroes_colab['fim_sexta'] / df_padroes_colab['total_spells']\n\n# Flag: > 50% dos spells come√ßam √† segunda OU terminam √† sexta\ndf_padroes_colab['flag_weekend_pattern'] = (\n    (df_padroes_colab['prop_inicio_segunda'] > 0.5) | \n    (df_padroes_colab['prop_fim_sexta'] > 0.5)\n) & (df_padroes_colab['total_spells'] >= 3)  # M√≠nimo 3 spells para ser significativo\n\n# Adicionar nomes\ndf_padroes_colab = df_padroes_colab.merge(\n    df[['login_colaborador', 'nome_colaborador']].drop_duplicates(),\n    on='login_colaborador'\n)\n\nn_flagged = df_padroes_colab['flag_weekend_pattern'].sum()\nprint(f'\\nüö© COLABORADORES COM PADR√ÉO SUSPEITO: {n_flagged}')\nprint(f'   (>50% spells come√ßam 2¬™ OU terminam 6¬™, com m√≠nimo 3 spells)')\n\nif n_flagged > 0:\n    print(f'\\nTop 10:')\n    top_weekend = df_padroes_colab[df_padroes_colab['flag_weekend_pattern']].nlargest(10, 'total_spells')[[\n        'nome_colaborador', 'total_spells', 'inicio_segunda', 'fim_sexta', \n        'prop_inicio_segunda', 'prop_fim_sexta'\n    ]].copy()\n    top_weekend['prop_inicio_segunda'] = top_weekend['prop_inicio_segunda'].apply(lambda x: f'{x*100:.1f}%')\n    top_weekend['prop_fim_sexta'] = top_weekend['prop_fim_sexta'].apply(lambda x: f'{x*100:.1f}%')\n    print(top_weekend.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.2 Padr√µes de Ponte - Aus√™ncias adjacentes a feriados\nprint('\\nAnalisando padr√µes de ponte (adjacentes a feriados)...')\n\n# NOTA: Precisamos de uma lista de feriados\n# Para demonstra√ß√£o, vamos criar feriados t√≠picos portugueses\nimport pandas as pd\nfrom datetime import datetime\n\n# Feriados fixos comuns (exemplo para um ano)\nano_min = df['Data'].min().year\nano_max = df['Data'].max().year\n\nferiados = []\nfor ano in range(ano_min, ano_max + 1):\n    feriados.extend([\n        datetime(ano, 1, 1),   # Ano Novo\n        datetime(ano, 4, 25),  # 25 de Abril\n        datetime(ano, 5, 1),   # Dia do Trabalhador\n        datetime(ano, 6, 10),  # Dia de Portugal\n        datetime(ano, 8, 15),  # Assun√ß√£o\n        datetime(ano, 10, 5),  # Implanta√ß√£o da Rep√∫blica\n        datetime(ano, 11, 1),  # Todos os Santos\n        datetime(ano, 12, 1),  # Restaura√ß√£o da Independ√™ncia\n        datetime(ano, 12, 8),  # Imaculada Concei√ß√£o\n        datetime(ano, 12, 25), # Natal\n    ])\n\nferiados = pd.to_datetime(feriados)\n\nprint(f'   Feriados considerados: {len(feriados)}')\n\n# Identificar spells que come√ßam/terminam adjacentes a feriados\ndef is_adjacent_to_holiday(date, holidays, tolerance=1):\n    \"\"\"Verifica se data est√° a ¬±tolerance dias de um feriado\"\"\"\n    for holiday in holidays:\n        diff = abs((date - holiday).days)\n        if diff <= tolerance:\n            return True\n    return False\n\ndf_spells['inicio_adjacente_feriado'] = df_spells['data_inicio'].apply(\n    lambda x: is_adjacent_to_holiday(x, feriados)\n)\n\ndf_spells['fim_adjacente_feriado'] = df_spells['data_fim'].apply(\n    lambda x: is_adjacent_to_holiday(x, feriados)\n)\n\nn_inicio_adj = df_spells['inicio_adjacente_feriado'].sum()\nn_fim_adj = df_spells['fim_adjacente_feriado'].sum()\n\nprint(f'\\nüìä PADR√ïES DE PONTE')\nprint(f'   Spells que come√ßam adjacentes a feriado: {n_inicio_adj:,} ({n_inicio_adj/len(df_spells)*100:.2f}%)')\nprint(f'   Spells que terminam adjacentes a feriado: {n_fim_adj:,} ({n_fim_adj/len(df_spells)*100:.2f}%)')\n\n# Por colaborador\ndf_ponte_colab = df_spells.groupby('login_colaborador').agg({\n    'spell_id': 'count',\n    'inicio_adjacente_feriado': 'sum',\n    'fim_adjacente_feriado': 'sum'\n}).reset_index()\n\ndf_ponte_colab.columns = ['login_colaborador', 'total_spells', 'inicio_adj_feriado', 'fim_adj_feriado']\ndf_ponte_colab['prop_ponte'] = (\n    (df_ponte_colab['inicio_adj_feriado'] + df_ponte_colab['fim_adj_feriado']) / \n    (df_ponte_colab['total_spells'] * 2)\n)\n\n# Flag: > 40% adjacentes a feriados\ndf_ponte_colab['flag_bridge_pattern'] = (\n    (df_ponte_colab['prop_ponte'] > 0.4) & \n    (df_ponte_colab['total_spells'] >= 3)\n)\n\ndf_ponte_colab = df_ponte_colab.merge(\n    df[['login_colaborador', 'nome_colaborador']].drop_duplicates(),\n    on='login_colaborador'\n)\n\nn_flagged_ponte = df_ponte_colab['flag_bridge_pattern'].sum()\nprint(f'\\nüö© COLABORADORES COM PADR√ÉO DE PONTE SUSPEITO: {n_flagged_ponte}')\n\nif n_flagged_ponte > 0:\n    print(f'\\nTop 10:')\n    top_ponte = df_ponte_colab[df_ponte_colab['flag_bridge_pattern']].nlargest(10, 'total_spells')[[\n        'nome_colaborador', 'total_spells', 'inicio_adj_feriado', 'fim_adj_feriado'\n    ]]\n    print(top_ponte.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.3 Outlier Detection - Colaboradores estatisticamente fora do padr√£o\nprint('\\nDete√ß√£o de outliers estat√≠sticos...')\n\nfrom scipy import stats\n\n# Z-score para v√°rias m√©tricas\ndf_outliers = df_bradford.copy()\n\n# Calcular Z-scores\ndf_outliers['z_num_spells'] = stats.zscore(df_outliers['num_spells'])\ndf_outliers['z_total_dias'] = stats.zscore(df_outliers['total_dias_ausentes'])\ndf_outliers['z_bradford'] = stats.zscore(df_outliers['bradford_score'])\n\n# Outlier se Z-score > 3 em qualquer m√©trica\ndf_outliers['is_outlier'] = (\n    (abs(df_outliers['z_num_spells']) > 3) |\n    (abs(df_outliers['z_total_dias']) > 3) |\n    (abs(df_outliers['z_bradford']) > 3)\n)\n\nn_outliers = df_outliers['is_outlier'].sum()\nprint(f'\\nüéØ OUTLIERS DETETADOS: {n_outliers}')\nprint(f'   ({n_outliers / len(df_outliers) * 100:.2f}% dos colaboradores)')\n\nif n_outliers > 0:\n    print(f'\\nOutliers (Z-score > 3 em alguma m√©trica):')\n    outliers = df_outliers[df_outliers['is_outlier']][\n        ['nome_colaborador', 'num_spells', 'total_dias_ausentes', 'bradford_score',\n         'z_num_spells', 'z_total_dias', 'z_bradford']\n    ].nlargest(15, 'z_bradford')\n    \n    for col in ['z_num_spells', 'z_total_dias', 'z_bradford']:\n        outliers[col] = outliers[col].apply(lambda x: f'{x:.2f}')\n    \n    print(outliers.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n## 7. AN√ÅLISE DE COHORTS (por Data de Ingresso)\n\n**Objetivo**: Investigar se colaboradores mais novos t√™m taxas de absentismo diferentes.\n\n**Hip√≥teses a testar**:\n- Novos colaboradores podem ter mais aus√™ncias (adapta√ß√£o, problemas iniciais)\n- Ou menos aus√™ncias (honeymoon period, medo de consequ√™ncias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.1 An√°lise de Cohorts\nprint('Analisando cohorts por data de ingresso...')\n\n# NOTA: Precisamos da data de ingresso de cada colaborador\n# Se n√£o existir no dataset, criar campo sint√©tico para demonstra√ß√£o\nif 'data_ingresso' not in df.columns:\n    print('\\n‚ö†Ô∏è  Campo \"data_ingresso\" n√£o encontrado no dataset.')\n    print('   Criando datas sint√©ticas para demonstra√ß√£o...')\n    \n    # Criar datas aleat√≥rias entre 2020 e data_inicio\n    np.random.seed(42)\n    colaboradores = df['login_colaborador'].unique()\n    data_inicio_empresa = pd.to_datetime('2020-01-01')\n    data_fim_contratacao = df['Data'].min()\n    \n    datas_ingresso = pd.DataFrame({\n        'login_colaborador': colaboradores,\n        'data_ingresso': pd.to_datetime(\n            np.random.randint(\n                data_inicio_empresa.value // 10**9,\n                data_fim_contratacao.value // 10**9,\n                size=len(colaboradores)\n            ),\n            unit='s'\n        )\n    })\n    \n    df = df.merge(datas_ingresso, on='login_colaborador', how='left')\n    print(f'   ‚úì Datas de ingresso criadas para {len(colaboradores):,} colaboradores')\n\n# Calcular tenure (tempo na empresa)\nreferencia_date = df['Data'].max()\ndf['tenure_dias'] = (referencia_date - df['data_ingresso']).dt.days\n\n# Categorizar por tenure\ndf['cohort'] = pd.cut(\n    df['tenure_dias'],\n    bins=[0, 180, 365, 730, 1095, float('inf')],\n    labels=['0-6 meses', '6-12 meses', '1-2 anos', '2-3 anos', '>3 anos']\n)\n\nprint(f'\\nDistribui√ß√£o de colaboradores por cohort:')\nprint(df.groupby('cohort')['login_colaborador'].nunique())\n\n# Calcular m√©tricas por cohort\ndf_cohort = df_bradford.merge(\n    df[['login_colaborador', 'data_ingresso', 'tenure_dias', 'cohort']].drop_duplicates(),\n    on='login_colaborador',\n    how='left'\n)\n\ncohort_metrics = df_cohort.groupby('cohort').agg({\n    'login_colaborador': 'count',\n    'num_spells': 'mean',\n    'total_dias_ausentes': 'mean',\n    'mean_spell_duration': 'mean',\n    'bradford_score': 'mean'\n}).reset_index()\n\ncohort_metrics.columns = [\n    'Cohort', 'N Colaboradores', 'M√©dia Spells', 'M√©dia Dias Ausentes',\n    'M√©dia Dura√ß√£o Spell', 'M√©dia Bradford'\n]\n\nprint(f'\\nüìä M√âTRICAS POR COHORT:')\nprint(cohort_metrics.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n## 8. CLUSTERING DE PERFIS DE ABSENTISMO\n\n**Objetivo**: Segmentar colaboradores em grupos homog√©neos por comportamento de absentismo.\n\n**Features para clustering**:\n- N√∫mero de spells (frequ√™ncia)\n- Total de dias ausentes\n- Dura√ß√£o m√©dia de spell\n- Bradford score\n- Propor√ß√£o de short-term spells\n\n**Algoritmo**: K-Means (3-5 clusters t√≠picos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.1 Preparar dados para clustering\nprint('Preparando features para clustering...')\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\n\n# Selecionar features\nfeatures_clustering = [\n    'num_spells',\n    'total_dias_ausentes', \n    'mean_spell_duration',\n    'bradford_score',\n    'num_short_term_spells'\n]\n\ndf_cluster = df_bradford[['login_colaborador', 'nome_colaborador'] + features_clustering].copy()\n\n# Normalizar features (importante para K-Means)\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(df_cluster[features_clustering])\n\nprint(f'\\n‚úì Features preparadas: {X_scaled.shape}')\n\n# Determinar n√∫mero ideal de clusters (Elbow Method)\nprint('\\nCalculando Elbow Method (K=2 a K=8)...')\ninertias = []\nK_range = range(2, 9)\n\nfor k in K_range:\n    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n    kmeans.fit(X_scaled)\n    inertias.append(kmeans.inertia_)\n\n# Visualizar Elbow\nfig = go.Figure()\nfig.add_trace(go.Scatter(\n    x=list(K_range),\n    y=inertias,\n    mode='lines+markers',\n    marker=dict(size=10)\n))\nfig.update_layout(\n    title='Elbow Method - Determinar K Ideal',\n    xaxis_title='N√∫mero de Clusters (K)',\n    yaxis_title='Inertia (Within-Cluster Sum of Squares)',\n    height=400\n)\nfig.show()\n\n# Aplicar K-Means com K=4 (t√≠pico para absentismo: baixo/moderado/alto/muito alto)\nK_optimal = 4\nprint(f'\\nAplicando K-Means com K={K_optimal}...')\n\nkmeans = KMeans(n_clusters=K_optimal, random_state=42, n_init=10)\ndf_cluster['cluster'] = kmeans.fit_predict(X_scaled)\n\nprint(f'\\n‚úì Clusters atribu√≠dos')\nprint(f'\\nDistribui√ß√£o por cluster:')\nprint(df_cluster['cluster'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.2 Interpretar clusters - Calcular m√©dias por cluster\nprint('\\n=== CARACTERIZA√á√ÉO DOS CLUSTERS ===')\n\ncluster_profiles = df_cluster.groupby('cluster')[features_clustering].mean()\n\n# Ordenar por bradford_score (do menor ao maior)\ncluster_profiles = cluster_profiles.sort_values('bradford_score')\n\n# Renomear clusters de forma interpret√°vel\ncluster_mapping = {}\nfor idx, (cluster_id, row) in enumerate(cluster_profiles.iterrows()):\n    if idx == 0:\n        cluster_mapping[cluster_id] = '1. BAIXO Absentismo'\n    elif idx == 1:\n        cluster_mapping[cluster_id] = '2. MODERADO Absentismo'\n    elif idx == 2:\n        cluster_mapping[cluster_id] = '3. ALTO Absentismo'\n    else:\n        cluster_mapping[cluster_id] = '4. MUITO ALTO Absentismo'\n\ndf_cluster['cluster_nome'] = df_cluster['cluster'].map(cluster_mapping)\n\n# Exibir perfis\nprint('\\nPERFIL M√âDIO POR CLUSTER:\\n')\nfor cluster_id, row in cluster_profiles.iterrows():\n    cluster_nome = cluster_mapping[cluster_id]\n    n_colab = (df_cluster['cluster'] == cluster_id).sum()\n    \n    print(f'{cluster_nome} (n={n_colab})')\n    print(f'   N¬∫ Spells: {row[\"num_spells\"]:.2f}')\n    print(f'   Total Dias Ausentes: {row[\"total_dias_ausentes\"]:.2f}')\n    print(f'   Dura√ß√£o M√©dia Spell: {row[\"mean_spell_duration\"]:.2f} dias')\n    print(f'   Bradford Score: {row[\"bradford_score\"]:.2f}')\n    print(f'   Short-term Spells: {row[\"num_short_term_spells\"]:.2f}')\n    print()\n\n# Exportar colaboradores com clusters\ndf_cluster_export = df_cluster[[\n    'nome_colaborador', 'cluster_nome', 'num_spells', 'total_dias_ausentes',\n    'mean_spell_duration', 'bradford_score'\n]].sort_values(['cluster_nome', 'bradford_score'], ascending=[True, False])\n\ndf_cluster_export.to_excel('colaboradores_clusters_absentismo.xlsx', index=False)\nprint('‚úì Clusters exportados para: colaboradores_clusters_absentismo.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.3 Visualizar clusters em 2D (PCA)\nprint('\\nVisualizando clusters em 2D (PCA)...')\n\nfrom sklearn.decomposition import PCA\n\n# Reduzir para 2 dimens√µes\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X_scaled)\n\ndf_cluster['PC1'] = X_pca[:, 0]\ndf_cluster['PC2'] = X_pca[:, 1]\n\nprint(f'Vari√¢ncia explicada: PC1={pca.explained_variance_ratio_[0]*100:.2f}%, PC2={pca.explained_variance_ratio_[1]*100:.2f}%')\n\n# Scatter plot\nfig = go.Figure()\n\nfor cluster_nome in sorted(df_cluster['cluster_nome'].unique()):\n    df_temp = df_cluster[df_cluster['cluster_nome'] == cluster_nome]\n    \n    fig.add_trace(go.Scatter(\n        x=df_temp['PC1'],\n        y=df_temp['PC2'],\n        mode='markers',\n        name=cluster_nome,\n        text=df_temp['nome_colaborador'],\n        marker=dict(size=8, opacity=0.7),\n        hovertemplate='<b>%{text}</b><br>PC1: %{x:.2f}<br>PC2: %{y:.2f}<extra></extra>'\n    ))\n\nfig.update_layout(\n    title='Clusters de Absentismo (PCA 2D)',\n    xaxis_title=f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}% var)',\n    yaxis_title=f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}% var)',\n    height=600\n)\n\nfig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n## 9. NETWORK ANALYSIS (Coincid√™ncias de Aus√™ncias)\n\n**Objetivo**: Detetar padr√µes de aus√™ncias **simult√¢neas** que possam indicar:\n- Eventos locais (gripe, problema na opera√ß√£o)\n- Problemas de equipa\n- Coincid√™ncias suspeitas\n\n**M√©todo**: Identificar dias com m√∫ltiplos colaboradores ausentes na mesma opera√ß√£o."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 9.1 Intui√ß√£o e Objetivos\n\n**Network Analysis** transforma o problema \"quem faltou em que dias\" numa **rede (grafo)** onde:\n- **N√≥** = Colaborador\n- **Aresta** = Conex√£o entre dois colaboradores que faltaram no mesmo dia\n\n**Objetivos**:\n1. Descobrir **pares/grupos** que faltam juntos com frequ√™ncia an√≥mala\n2. Identificar **equipas/opera√ß√µes** com alta co-incid√™ncia\n3. Detetar **eventos pontuais** vs **padr√µes persistentes**\n4. **Priorizar investiga√ß√µes** RH\n\n**Metodologia**:\n- Matriz de **co-ocorr√™ncia** (quantas vezes A e B faltaram juntos)\n- **Testes estat√≠sticos** (hipergeom√©trico) para filtrar coincid√™ncias significativas\n- **Community detection** (Louvain) para encontrar clusters\n- **Centralidade** para priorizar investiga√ß√µes\n- **Visualiza√ß√£o interativa** (pyvis)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 9.2 Preparar dados: Matriz Bipartida (Colaboradores √ó Dias)\nprint('=== NETWORK ANALYSIS: CO-OCORR√äNCIA DE AUS√äNCIAS ===')\nprint('\\nPreparando matriz bipartida...')\n\n# Filtrar apenas aus√™ncias (excluir Trabalho Pago, Atraso)\ndf_ausencias_network = df[\n    df['Nivel 1'].isin(['Falta Justificada', 'Falta Injustificada', 'Aus√™ncia'])\n].copy()\n\nprint(f'   Registos de aus√™ncia: {len(df_ausencias_network):,}')\nprint(f'   Colaboradores: {df_ausencias_network[\"login_colaborador\"].nunique():,}')\nprint(f'   Dias √∫nicos: {df_ausencias_network[\"Data\"].nunique():,}')\n\n# Criar identificador √∫nico de dia\ndf_ausencias_network['date_id'] = df_ausencias_network['Data'].astype(str)\n\n# Lista de colaboradores e dias\ncolaboradores = sorted(df_ausencias_network['login_colaborador'].unique())\ndias = sorted(df_ausencias_network['date_id'].unique())\n\nprint(f'\\nDimens√£o da matriz: {len(colaboradores):,} colaboradores √ó {len(dias):,} dias')\nprint(f'   Esparsidade: {len(df_ausencias_network) / (len(colaboradores) * len(dias)) * 100:.2f}%')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 9.3 Criar Matriz de Co-ocorr√™ncia (C = B √ó B.T)\nprint('\\nCriando matriz de co-ocorr√™ncia...')\nprint('   (Isto pode demorar alguns minutos para datasets grandes)')\n\nfrom scipy.sparse import csr_matrix, lil_matrix\nimport numpy as np\n\n# Criar dicion√°rios de √≠ndice\ncolab_to_idx = {c: i for i, c in enumerate(colaboradores)}\ndate_to_idx = {d: i for i, d in enumerate(dias)}\n\n# Criar matriz bipartida esparsa (colaboradores √ó dias)\n# B[i,j] = 1 se colaborador i esteve ausente no dia j\nrows = []\ncols = []\n\nfor _, row in df_ausencias_network.iterrows():\n    colab_idx = colab_to_idx[row['login_colaborador']]\n    date_idx = date_to_idx[row['date_id']]\n    rows.append(colab_idx)\n    cols.append(date_idx)\n\nB = csr_matrix(\n    (np.ones(len(rows)), (rows, cols)),\n    shape=(len(colaboradores), len(dias)),\n    dtype=np.int8\n)\n\nprint(f'   Matriz B criada: {B.shape}')\nprint(f'   Mem√≥ria: {B.data.nbytes / 1024 / 1024:.2f} MB')\n\n# Calcular co-ocorr√™ncia: C = B √ó B.T\n# C[i,j] = n√∫mero de dias em que colaboradores i e j estiveram AMBOS ausentes\nprint('\\n   Calculando C = B √ó B.T...')\nC = B @ B.T\n\n# Converter para array denso (se couber em mem√≥ria)\n# Se dataset muito grande, trabalhar com sparse\nif len(colaboradores) < 5000:\n    C_dense = C.toarray()\n    print(f'   Matriz C: {C_dense.shape} (densa)')\nelse:\n    C_dense = None\n    print(f'   Matriz C: {C.shape} (esparsa - dataset grande)')\n\nprint('\\n‚úì Matriz de co-ocorr√™ncia criada')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 9.4 Teste Estat√≠stico: P-values (Distribui√ß√£o Hipergeom√©trica)\nprint('\\nCalculando p-values para coincid√™ncias...')\n\nfrom scipy.stats import hypergeom\n\n# Para cada par (i,j), calcular:\n# H0: aus√™ncias s√£o aleat√≥rias ao longo de N dias\n# N = total de dias\n# K = dias ausentes de i (C[i,i])\n# M = dias ausentes de j (C[j,j])\n# k = observado (C[i,j])\n# P(X >= k) sob H0\n\nN_dias = len(dias)\n\n# Dias ausentes por colaborador (diagonal de C)\ndias_ausentes = np.diag(C.toarray()) if C_dense is None else np.diag(C_dense)\n\nprint(f'   N = {N_dias} dias totais')\nprint(f'   M√©dia de dias ausentes por colaborador: {dias_ausentes.mean():.2f}')\n\n# Criar lista de pares significativos\n# (s√≥ calcular para pares com k > 0 para efici√™ncia)\npares_significativos = []\n\nprint('\\n   Testando pares com co-ocorr√™ncias...')\n\n# Threshold m√≠nimo de co-ocorr√™ncias para considerar\nmin_cooccur = 2  # Pelo menos 2 dias juntos\n\nn_pares_testados = 0\nif C_dense is not None:\n    # Trabalhar com matriz densa\n    for i in range(len(colaboradores)):\n        if i % 500 == 0 and i > 0:\n            print(f'      Processado {i:,}/{len(colaboradores):,} colaboradores...')\n\n        for j in range(i+1, len(colaboradores)):\n            k_obs = C_dense[i, j]\n\n            if k_obs >= min_cooccur:\n                n_pares_testados += 1\n\n                K_i = dias_ausentes[i]\n                M_j = dias_ausentes[j]\n\n                # P-value: P(X >= k_obs) sob hipergeom√©trica\n                # hypergeom(M, n, N) onde M=N_dias, n=K_i, N=M_j\n                # Survival function: P(X >= k)\n                try:\n                    pval = hypergeom.sf(k_obs - 1, N_dias, K_i, M_j)\n                except:\n                    pval = 1.0  # Se erro, considerar n√£o significativo\n\n                pares_significativos.append({\n                    'colaborador_i': colaboradores[i],\n                    'colaborador_j': colaboradores[j],\n                    'dias_i': K_i,\n                    'dias_j': M_j,\n                    'cooccur': k_obs,\n                    'p_value': pval\n                })\nelse:\n    # Trabalhar com matriz esparsa (mais lento mas menos mem√≥ria)\n    C_lil = C.tolil()\n    for i in range(len(colaboradores)):\n        if i % 500 == 0 and i > 0:\n            print(f'      Processado {i:,}/{len(colaboradores):,} colaboradores...')\n\n        for j in range(i+1, len(colaboradores)):\n            k_obs = C_lil[i, j]\n\n            if k_obs >= min_cooccur:\n                n_pares_testados += 1\n\n                K_i = C_lil[i, i]\n                M_j = C_lil[j, j]\n\n                try:\n                    pval = hypergeom.sf(k_obs - 1, N_dias, K_i, M_j)\n                except:\n                    pval = 1.0\n\n                pares_significativos.append({\n                    'colaborador_i': colaboradores[i],\n                    'colaborador_j': colaboradores[j],\n                    'dias_i': K_i,\n                    'dias_j': M_j,\n                    'cooccur': k_obs,\n                    'p_value': pval\n                })\n\ndf_pares = pd.DataFrame(pares_significativos)\n\nprint(f'\\n‚úì Testados {n_pares_testados:,} pares com ‚â•{min_cooccur} co-ocorr√™ncias')\nprint(f'   Total de pares potenciais: {len(colaboradores) * (len(colaboradores)-1) // 2:,}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 9.5 Corre√ß√£o FDR (False Discovery Rate)\nprint('\\nAplicando corre√ß√£o FDR...')\n\nfrom statsmodels.stats.multitest import multipletests\n\nif len(df_pares) > 0:\n    # Aplicar Benjamini-Hochberg FDR\n    reject, pvals_adj, _, _ = multipletests(\n        df_pares['p_value'],\n        alpha=0.05,\n        method='fdr_bh'\n    )\n\n    df_pares['p_adj'] = pvals_adj\n    df_pares['significant'] = reject\n\n    n_sig = reject.sum()\n    print(f'   Pares significativos (FDR < 0.05): {n_sig:,}')\n    print(f'   ({n_sig / len(df_pares) * 100:.2f}% dos pares testados)')\n\n    # Filtrar apenas significativos\n    df_pares_sig = df_pares[df_pares['significant']].copy()\n\n    # Ordenar por co-ocorr√™ncias (descendente)\n    df_pares_sig = df_pares_sig.sort_values('cooccur', ascending=False).reset_index(drop=True)\n\n    print(f'\\nüìä TOP 20 PARES COM COINCID√äNCIAS SIGNIFICATIVAS:')\n    print('='*100)\n\n    top20_pares = df_pares_sig.head(20).copy()\n\n    # Adicionar nomes\n    top20_pares = top20_pares.merge(\n        df[['login_colaborador', 'nome_colaborador']].drop_duplicates(),\n        left_on='colaborador_i',\n        right_on='login_colaborador',\n        how='left'\n    ).rename(columns={'nome_colaborador': 'nome_i'}).drop('login_colaborador', axis=1)\n\n    top20_pares = top20_pares.merge(\n        df[['login_colaborador', 'nome_colaborador']].drop_duplicates(),\n        left_on='colaborador_j',\n        right_on='login_colaborador',\n        how='left'\n    ).rename(columns={'nome_colaborador': 'nome_j'}).drop('login_colaborador', axis=1)\n\n    # Adicionar opera√ß√£o (se dispon√≠vel)\n    if 'operacao' in df.columns:\n        top20_pares = top20_pares.merge(\n            df[['login_colaborador', 'operacao']].drop_duplicates(),\n            left_on='colaborador_i',\n            right_on='login_colaborador',\n            how='left'\n        ).rename(columns={'operacao': 'operacao_i'}).drop('login_colaborador', axis=1)\n\n        top20_pares = top20_pares.merge(\n            df[['login_colaborador', 'operacao']].drop_duplicates(),\n            left_on='colaborador_j',\n            right_on='login_colaborador',\n            how='left'\n        ).rename(columns={'operacao': 'operacao_j'}).drop('login_colaborador', axis=1)\n\n    # Exibir\n    for idx, row in top20_pares.iterrows():\n        print(f\"\\n{idx+1}. {row['nome_i']} + {row['nome_j']}\")\n        print(f\"   Co-ocorr√™ncias: {row['cooccur']} dias\")\n        print(f\"   P-value ajustado: {row['p_adj']:.2e}\")\n        if 'operacao_i' in row:\n            print(f\"   Opera√ß√µes: {row['operacao_i']} + {row['operacao_j']}\")\n\n    # Exportar para Excel\n    top20_pares.to_excel('network_top20_pares_significativos.xlsx', index=False)\n    print('\\n‚úì Top 20 pares exportados para: network_top20_pares_significativos.xlsx')\n\nelse:\n    print('   Nenhum par encontrado')\n    df_pares_sig = pd.DataFrame()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 9.6 Community Detection (Louvain Algorithm)\nprint('\\nDete√ß√£o de comunidades (Louvain)...')\n\nif len(df_pares_sig) > 0:\n    import networkx as nx\n    from networkx.algorithms import community\n\n    # Criar grafo apenas com arestas significativas\n    G = nx.Graph()\n\n    # Adicionar n√≥s\n    G.add_nodes_from(colaboradores)\n\n    # Adicionar arestas (ponderadas por co-ocorr√™ncias)\n    for _, row in df_pares_sig.iterrows():\n        G.add_edge(\n            row['colaborador_i'],\n            row['colaborador_j'],\n            weight=row['cooccur'],\n            p_adj=row['p_adj']\n        )\n\n    print(f'   Grafo criado: {G.number_of_nodes():,} n√≥s, {G.number_of_edges():,} arestas')\n\n    # Aplicar Louvain\n    print('   Aplicando algoritmo de Louvain...')\n    communities_generator = community.greedy_modularity_communities(G, weight='weight')\n    communities_list = list(communities_generator)\n\n    print(f'   Comunidades detetadas: {len(communities_list)}')\n\n    # Criar mapeamento colaborador -> comunidade\n    colab_to_community = {}\n    for comm_id, comm_members in enumerate(communities_list):\n        for member in comm_members:\n            colab_to_community[member] = comm_id\n\n    # Estat√≠sticas por comunidade\n    print(f'\\nüìä COMUNIDADES (clusters de co-aus√™ncias):')\n    print('='*80)\n\n    for comm_id, comm_members in enumerate(sorted(communities_list, key=len, reverse=True)[:10]):\n        print(f'\\nComunidade {comm_id + 1}: {len(comm_members)} membros')\n\n        # Nomes (primeiros 5)\n        member_names = []\n        for member in list(comm_members)[:5]:\n            nome = df[df['login_colaborador'] == member]['nome_colaborador'].iloc[0]\n            member_names.append(nome)\n\n        print(f'   Membros (sample): {', '.join(member_names)}...')\n\n        # Opera√ß√µes predominantes\n        if 'operacao' in df.columns:\n            ops = df[df['login_colaborador'].isin(comm_members)]['operacao'].value_counts().head(3)\n            print(f'   Opera√ß√µes predominantes:')\n            for op, count in ops.items():\n                print(f'      - {op}: {count} registos')\n\n    # Salvar comunidades\n    df_communities = pd.DataFrame([\n        {'login_colaborador': colab, 'community_id': comm_id}\n        for colab, comm_id in colab_to_community.items()\n    ])\n\n    df_communities = df_communities.merge(\n        df[['login_colaborador', 'nome_colaborador']].drop_duplicates(),\n        on='login_colaborador'\n    )\n\n    df_communities.to_excel('network_comunidades.xlsx', index=False)\n    print('\\n‚úì Comunidades exportadas para: network_comunidades.xlsx')\n\nelse:\n    print('   Nenhuma aresta significativa - comunidades n√£o podem ser detetadas')\n    G = None\n    colab_to_community = {}"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 9.7 Centralidade: Priorizar Investiga√ß√µes\nprint('\\nCalculando m√©tricas de centralidade...')\n\nif G is not None and G.number_of_edges() > 0:\n    # Degree centrality (n√∫mero de conex√µes)\n    degree_cent = nx.degree_centrality(G)\n\n    # Betweenness centrality (ponte entre clusters)\n    print('   Calculando betweenness centrality (pode demorar)...')\n    betweenness_cent = nx.betweenness_centrality(G, weight='weight')\n\n    # Closeness centrality\n    # print('   Calculando closeness centrality...')\n    # closeness_cent = nx.closeness_centrality(G, distance='weight')\n\n    # Weighted degree (soma dos pesos)\n    weighted_degree = dict(G.degree(weight='weight'))\n\n    # Criar DataFrame\n    df_centrality = pd.DataFrame({\n        'login_colaborador': list(degree_cent.keys()),\n        'degree_centrality': list(degree_cent.values()),\n        'betweenness_centrality': list(betweenness_cent.values()),\n        # 'closeness_centrality': list(closeness_cent.values()),\n        'weighted_degree': [weighted_degree[c] for c in degree_cent.keys()]\n    })\n\n    # Adicionar nomes\n    df_centrality = df_centrality.merge(\n        df[['login_colaborador', 'nome_colaborador']].drop_duplicates(),\n        on='login_colaborador'\n    )\n\n    # Top 20 por degree\n    print(f'\\nüéØ TOP 20 COLABORADORES POR CENTRALIDADE (hubs de co-aus√™ncia):')\n    print('='*100)\n\n    top20_centrality = df_centrality.nlargest(20, 'degree_centrality')\n\n    for idx, row in top20_centrality.iterrows():\n        print(f\"{row['nome_colaborador']:40s} | Connections: {int(row['weighted_degree']):3d} | \"\n              f\"Degree: {row['degree_centrality']:.4f} | Betweenness: {row['betweenness_centrality']:.4f}\")\n\n    # Exportar\n    df_centrality.to_excel('network_centralidade.xlsx', index=False)\n    print('\\n‚úì M√©tricas de centralidade exportadas para: network_centralidade.xlsx')\n\nelse:\n    print('   Grafo vazio ou sem arestas')\n    df_centrality = pd.DataFrame()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 9.8 Visualiza√ß√£o Interativa (Pyvis)\nprint('\\nCriando visualiza√ß√£o interativa da rede...')\n\nif G is not None and G.number_of_edges() > 0:\n    try:\n        from pyvis.network import Network\n\n        # Limitar a top comunidades e top pares para legibilidade\n        # Filtrar apenas top 5 comunidades e top 50 pares\n        top_communities = sorted(communities_list, key=len, reverse=True)[:5]\n        top_community_ids = [i for i, comm in enumerate(communities_list) if comm in top_communities]\n\n        # N√≥s da top comunidades\n        top_nodes = set()\n        for comm in top_communities:\n            top_nodes.update(comm)\n\n        # Criar subgrafo\n        G_sub = G.subgraph(top_nodes).copy()\n\n        print(f'   Visualizando subgrafo: {G_sub.number_of_nodes()} n√≥s, {G_sub.number_of_edges()} arestas')\n\n        # Criar network pyvis\n        net = Network(height='750px', width='100%', bgcolor='#222222', font_color='white')\n\n        # Configura√ß√µes\n        net.barnes_hut(gravity=-5000, central_gravity=0.3, spring_length=100)\n\n        # Cores por comunidade\n        colors = ['#e6194b', '#3cb44b', '#ffe119', '#4363d8', '#f58231',\n                  '#911eb4', '#46f0f0', '#f032e6', '#bcf60c', '#fabebe']\n\n        # Adicionar n√≥s\n        for node in G_sub.nodes():\n            # Nome\n            nome = df[df['login_colaborador'] == node]['nome_colaborador'].iloc[0] if len(df[df['login_colaborador'] == node]) > 0 else node\n\n            # Comunidade\n            comm_id = colab_to_community.get(node, -1)\n            color = colors[comm_id % len(colors)] if comm_id >= 0 else '#cccccc'\n\n            # Tamanho proporcional ao degree\n            degree = G_sub.degree(node, weight='weight')\n            size = 10 + degree * 2\n\n            net.add_node(\n                node,\n                label=nome[:20],  # Limitar label\n                title=f\"{nome}\\nComunidade: {comm_id+1}\\nDegree: {degree}\",\n                color=color,\n                size=size\n            )\n\n        # Adicionar arestas\n        for edge in G_sub.edges(data=True):\n            net.add_edge(\n                edge[0],\n                edge[1],\n                value=edge[2]['weight'],  # Espessura proporcional ao peso\n                title=f\"{edge[2]['weight']} co-ocorr√™ncias\"\n            )\n\n        # Salvar\n        net.save_graph('network_visualization.html')\n        print('\\n‚úì Visualiza√ß√£o interativa criada: network_visualization.html')\n        print('   (Abrir no browser para explorar)')\n\n    except ImportError:\n        print('\\n‚ö†Ô∏è  pyvis n√£o instalado. Instalando...')\n        import subprocess\n        subprocess.run(['pip', 'install', 'pyvis', '-q'])\n        print('   Por favor, executar esta c√©lula novamente')\n\nelse:\n    print('   Grafo vazio - visualiza√ß√£o n√£o criada')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 9.9 Din√¢mica Temporal: Evolu√ß√£o dos Pares Significativos\nprint('\\nAnalisando din√¢mica temporal da rede...')\n\nif len(df_pares_sig) > 0:\n    # Para cada par significativo, encontrar QUANDO aconteceram as co-aus√™ncias\n\n    # Pegar top 10 pares\n    top_pares = df_pares_sig.head(10)\n\n    # Para cada par, buscar datas\n    timeline_data = []\n\n    for _, pair in top_pares.iterrows():\n        colab_i = pair['colaborador_i']\n        colab_j = pair['colaborador_j']\n\n        # Encontrar dias em que AMBOS faltaram\n        dias_i = set(df_ausencias_network[df_ausencias_network['login_colaborador'] == colab_i]['Data'])\n        dias_j = set(df_ausencias_network[df_ausencias_network['login_colaborador'] == colab_j]['Data'])\n\n        dias_comuns = sorted(dias_i & dias_j)\n\n        # Agrupar por m√™s\n        for data in dias_comuns:\n            mes = data.to_period('M').to_timestamp()\n            timeline_data.append({\n                'par': f\"{pair['nome_i'][:15]}...\\n{pair['nome_j'][:15]}...\",\n                'mes': mes,\n                'cooccur_count': 1\n            })\n\n    if timeline_data:\n        df_timeline = pd.DataFrame(timeline_data)\n\n        # Agregar por par e m√™s\n        df_timeline_agg = df_timeline.groupby(['par', 'mes'])['cooccur_count'].sum().reset_index()\n\n        # Plot\n        fig = px.line(\n            df_timeline_agg,\n            x='mes',\n            y='cooccur_count',\n            color='par',\n            title='Evolu√ß√£o Temporal: Co-ocorr√™ncias dos Top 10 Pares',\n            labels={'mes': 'M√™s', 'cooccur_count': 'N¬∫ Co-ocorr√™ncias', 'par': 'Par'},\n            height=600\n        )\n\n        fig.update_traces(mode='lines+markers')\n        fig.update_layout(hovermode='x unified')\n\n        fig.show()\n\n        print('‚úì Timeline criada')\n    else:\n        print('   Sem dados suficientes para timeline')\nelse:\n    print('   Nenhum par significativo para an√°lise temporal')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n## 10. EVENT DETECTION & ANOMALY DETECTION\n\n**Objetivo**: Monitorar evolu√ß√£o temporal e detetar **mudan√ßas de padr√£o** (changepoints).\n\n**M√©todos**:\n- **U-Chart**: Control chart para taxa por unidade de tempo\n- **Rolling statistics**: M√©dia m√≥vel e desvio padr√£o\n- **Anomaly detection**: Per√≠odos estatisticamente an√≥malos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10.1 U-Chart - Taxa de aus√™ncias por semana\nprint('Criando U-Chart (control chart)...')\n\n# Agrupar por semana\ndf_ausencias['semana'] = df_ausencias['Data'].dt.to_period('W').astype(str)\n\nausencias_semana = df_ausencias.groupby('semana').size().reset_index(name='num_ausencias')\nausencias_semana['semana_dt'] = pd.to_datetime(ausencias_semana['semana'].str.split('/').str[0])\n\n# Calcular colaboradores ativos por semana (para normalizar)\ncolab_semana = df.groupby(df['Data'].dt.to_period('W').astype(str))['login_colaborador'].nunique()\nausencias_semana['colaboradores_ativos'] = ausencias_semana['semana'].map(colab_semana)\n\n# Taxa de aus√™ncias por colaborador\nausencias_semana['taxa_ausencia_colab'] = (\n    ausencias_semana['num_ausencias'] / ausencias_semana['colaboradores_ativos']\n)\n\n# Limites de controle (U-chart)\n# UCL/LCL = mean ¬± 3*sqrt(mean/n)\nmean_taxa = ausencias_semana['taxa_ausencia_colab'].mean()\nausencias_semana['ucl'] = mean_taxa + 3 * np.sqrt(mean_taxa / ausencias_semana['colaboradores_ativos'])\nausencias_semana['lcl'] = mean_taxa - 3 * np.sqrt(mean_taxa / ausencias_semana['colaboradores_ativos'])\nausencias_semana['lcl'] = ausencias_semana['lcl'].clip(lower=0)\n\n# Identificar semanas fora de controle\nausencias_semana['out_of_control'] = (\n    (ausencias_semana['taxa_ausencia_colab'] > ausencias_semana['ucl']) |\n    (ausencias_semana['taxa_ausencia_colab'] < ausencias_semana['lcl'])\n)\n\nn_out = ausencias_semana['out_of_control'].sum()\nprint(f'\\nüìä U-CHART')\nprint(f'   Semanas analisadas: {len(ausencias_semana)}')\nprint(f'   Taxa m√©dia: {mean_taxa:.3f} aus√™ncias/colaborador/semana')\nprint(f'   Semanas FORA DE CONTROLE: {n_out} ({n_out/len(ausencias_semana)*100:.2f}%)')\n\n# Visualizar U-Chart\nfig = go.Figure()\n\nfig.add_trace(go.Scatter(\n    x=ausencias_semana['semana_dt'],\n    y=ausencias_semana['taxa_ausencia_colab'],\n    mode='lines+markers',\n    name='Taxa Observada',\n    line=dict(color='blue'),\n    marker=dict(\n        size=6,\n        color=ausencias_semana['out_of_control'].map({True: 'red', False: 'blue'})\n    )\n))\n\nfig.add_trace(go.Scatter(\n    x=ausencias_semana['semana_dt'],\n    y=[mean_taxa] * len(ausencias_semana),\n    mode='lines',\n    name='M√©dia',\n    line=dict(color='green', dash='dash')\n))\n\nfig.add_trace(go.Scatter(\n    x=ausencias_semana['semana_dt'],\n    y=ausencias_semana['ucl'],\n    mode='lines',\n    name='UCL (Upper Control Limit)',\n    line=dict(color='red', dash='dot')\n))\n\nfig.add_trace(go.Scatter(\n    x=ausencias_semana['semana_dt'],\n    y=ausencias_semana['lcl'],\n    mode='lines',\n    name='LCL (Lower Control Limit)',\n    line=dict(color='red', dash='dot'),\n    fill='tonexty',\n    fillcolor='rgba(255,0,0,0.1)'\n))\n\nfig.update_layout(\n    title='U-Chart: Taxa de Aus√™ncias por Semana (Control Chart)',\n    xaxis_title='Semana',\n    yaxis_title='Aus√™ncias / Colaborador',\n    height=500,\n    hovermode='x unified'\n)\n\nfig.show()\n\nif n_out > 0:\n    print(f'\\nSemanas fora de controle:')\n    print(ausencias_semana[ausencias_semana['out_of_control']][\n        ['semana', 'num_ausencias', 'colaboradores_ativos', 'taxa_ausencia_colab', 'ucl']\n    ].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n## 11. VISUALIZA√á√ïES AVAN√áADAS\n\n### 11.1 Calendar Heatmap - Visualizar dias cr√≠ticos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11.1 Calendar Heatmap por Opera√ß√£o\nprint('Criando calendar heatmap...')\n\nif 'operacao' in df_ausencias.columns:\n    # Selecionar top 3 opera√ß√µes por n√∫mero de aus√™ncias\n    top_operacoes = df_ausencias['operacao'].value_counts().head(3).index.tolist()\n    \n    for operacao in top_operacoes:\n        print(f'\\nCriando heatmap para: {operacao}')\n        \n        df_op = df_ausencias[df_ausencias['operacao'] == operacao].copy()\n        \n        # Contar aus√™ncias por dia\n        ausencias_dia = df_op.groupby('Data').size().reset_index(name='num_ausencias')\n        \n        # Adicionar features para heatmap\n        ausencias_dia['ano'] = ausencias_dia['Data'].dt.year\n        ausencias_dia['semana'] = ausencias_dia['Data'].dt.isocalendar().week\n        ausencias_dia['dia_semana'] = ausencias_dia['Data'].dt.dayofweek\n        \n        # Pivot para heatmap: semanas x dias da semana\n        pivot = ausencias_dia.pivot_table(\n            index='semana',\n            columns='dia_semana',\n            values='num_ausencias',\n            aggfunc='sum'\n        ).fillna(0)\n        \n        # Criar heatmap\n        fig = go.Figure(data=go.Heatmap(\n            z=pivot.values,\n            x=['Segunda', 'Ter√ßa', 'Quarta', 'Quinta', 'Sexta', 'S√°bado', 'Domingo'],\n            y=pivot.index,\n            colorscale='Reds',\n            hoverongaps=False,\n            hovertemplate='Semana %{y}<br>%{x}<br>Aus√™ncias: %{z}<extra></extra>'\n        ))\n        \n        fig.update_layout(\n            title=f'Calendar Heatmap: Aus√™ncias - {operacao}',\n            xaxis_title='Dia da Semana',\n            yaxis_title='Semana do Ano',\n            height=600\n        )\n        \n        fig.show()\nelse:\n    print('\\n‚ö†Ô∏è  Campo \"operacao\" n√£o encontrado')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n## 12. S√çNTESE EXECUTIVA E A√á√ïES RECOMENDADAS\n\n**Esta sec√ß√£o consolida os principais insights e recomenda√ß√µes para a√ß√£o.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12.1 Dashboard Executivo\nprint('='*100)\nprint('S√çNTESE EXECUTIVA - AN√ÅLISE DE ABSENTISMO')\nprint('='*100)\n\nprint(f'\\nüéØ KPIs PRINCIPAIS')\nprint(f'   Taxa de Absentismo Global: {taxa_absentismo_global:.2f}%')\nprint(f'   Lost Time Rate: {lost_time_rate:.2f} dias/colaborador')\nprint(f'   Frequency Rate: {frequency_rate:.2f} spells/colaborador')\nprint(f'   Mean Spell Duration: {mean_spell_duration:.2f} dias')\n\nprint(f'\\nüö® ALERTAS CR√çTICOS')\nprint(f'   Colaboradores Bradford >500 (a√ß√£o disciplinar): {(df_bradford[\"bradford_score\"] > 500).sum()}')\nprint(f'   Colaboradores Bradford >900 (preocupa√ß√£o s√©ria): {(df_bradford[\"bradford_score\"] > 900).sum()}')\n\nif 'df_padroes_colab' in locals():\n    print(f'   Padr√£o Segunda/Sexta suspeito: {df_padroes_colab[\"flag_weekend_pattern\"].sum()} colaboradores')\n\nif 'df_ponte_colab' in locals():\n    print(f'   Padr√£o de Ponte suspeito: {df_ponte_colab[\"flag_bridge_pattern\"].sum()} colaboradores')\n\nprint(f'   Outliers estat√≠sticos: {n_outliers} colaboradores')\n\nif 'df_surtos' in locals():\n    print(f'   Dias com surto de aus√™ncias: {len(df_surtos)} dias')\n\nif 'ausencias_semana' in locals():\n    print(f'   Semanas fora de controle (U-Chart): {n_out} semanas')\n\nprint(f'\\nüìä PERFIS DE ABSENTISMO (Clusters)')\nif 'df_cluster' in locals():\n    for cluster in sorted(df_cluster['cluster_nome'].unique()):\n        n = (df_cluster['cluster_nome'] == cluster).sum()\n        pct = n / len(df_cluster) * 100\n        print(f'   {cluster}: {n} colaboradores ({pct:.1f}%)')\n\nprint(f'\\nüí° A√á√ïES RECOMENDADAS')\nprint(f'\\n1. IMEDIATAS (pr√≥ximas 2 semanas):')\nprint(f'   - Conversa com Top 20 Bradford Factor (>= posi√ß√£o de aviso escrito)')\nprint(f'   - Investigar {len(df_surtos) if \"df_surtos\" in locals() else 0} dias de surto identificados')\nprint(f'   - Rever casos de padr√£o segunda/sexta e ponte')\n\nprint(f'\\n2. CURTO PRAZO (pr√≥ximo m√™s):')\nprint(f'   - Implementar monitoriza√ß√£o cont√≠nua (U-Chart semanal)')\nprint(f'   - Definir pol√≠tica de follow-up por cluster')\nprint(f'   - An√°lise detalhada das {n_out if \"ausencias_semana\" in locals() else 0} semanas fora de controle')\n\nprint(f'\\n3. M√âDIO PRAZO (pr√≥ximos 3 meses):')\nprint(f'   - Programa de engagement para clusters alto/muito alto')\nprint(f'   - Investigar causas raiz por opera√ß√£o')\nprint(f'   - Implementar sistema de early warning (Bradford + padr√µes)')\n\nprint(f'\\n' + '='*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n## üìù NOTAS FINAIS\n\n**Ficheiros Gerados**:\n- `incompatibilidades_encontradas_v2.xlsx` - Casos de dados inconsistentes removidos\n- `matriz_compatibilidade_nivel2_v2.xlsx` - Regras de compatibilidade aplicadas\n- `bradford_factor_top20.xlsx` - Colaboradores priorit√°rios para RH\n- `colaboradores_clusters_absentismo.xlsx` - Segmenta√ß√£o completa\n- `dias_surto_ausencias.xlsx` - Dias com aus√™ncias an√≥malas\n\n**Pr√≥ximos Passos**:\n1. Validar resultados com RH\n2. Criar dashboard interativo (Power BI / Tableau)\n3. Implementar monitoriza√ß√£o cont√≠nua\n4. Desenvolver ferramenta Excel para consulta (Parte 2)\n\n**Refer√™ncias**:\n- Bradford Factor: Wikipedia, Call Centre Helper\n- Spell Analysis: Fitzgerald Human Resources\n- HR Analytics: AIHR (Academy to Innovate HR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}